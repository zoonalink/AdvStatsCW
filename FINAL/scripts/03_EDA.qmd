---
title: "Chemical Sample Classification Report"
#subtitle: "Exploratory Data"
author: "Petter LÃ¶vehagen"
date: "08 May 2023"

RVersion: 3.6.0 
RStudio: 2023.03.0+386 Cherry Blossom Release
Platform: "x86_64-w64-mingw32/x64 (64-bit)"
OS: "Windows 11 SE"

format: 
  docx: default
  html:
    toc: true
    toc-depth: 3
    toc-title: "Contents"
    toc-location: left
    code-fold: true
    code-line-numbers: true
    theme: spacelab
    html-math-method: mathjax
    highlight: "tango"
    #css: "../data/custom.css"
output:
  word_document:
    fig.retina: 300
prefer-docx: true
lang: "en-GB"
bibliography: "../data/AS_CW_References.bib"
csl: "../data/harvard-university-of-the-west-of-england.csl"

knitr:
  opts_chunk:
    include: false
    tidy: true
    echo: false
    warning: false
    message: false
    error: true
    out.width: '90%'
    #fig.width: 6
    #fig.height: 4
---

<!--# The 'chem_packages.R file installs/loads required packages for this section, sets a seed (for reproducibility) and loads flextable defaults. Check for further details. -->

```{r EDA_packages}
# load/install R packages/libraries
# set seed
# load flextable defaults

file <- "EDA"
source("chem_packages.R")


# what is currently loaded/attached:
#sessionInfo()

# for this section
# library(flextable)
# library(dplyr)
# library(ggplot2)
# library(cowplot)
# library(corrplot)
# library(GGally)
# library(psych)
# library(patchwork)

```

```{r load_data_from_csv}
# load train data set
train <-read.csv("../data/chem_train.csv")
# extract numeric columns
num_train <- train[, -21]

```

```{r train_structure}
# structure and summary

head(train)
dim(train)
str(train)
summary(train)
```

```{r stats_psych}
#summary of stats using 'psych'

describe(train)
```

<!--# Looking at the overall statistics of the dataset before any transformation, there are different scales between the variables.  Without knowing the units of measurement or anything else about the dataset, the data will need to be mean scaled/normalised to ensure that variables are on the same scale. -->

### Exploratory Data Analysis {#sec-exploratory-data-analysis}

Only the *train* dataset will be analysed in the EDA stage.

#### Overall Size and Shape {#sec-overall-statistics}

```{r train_statistics}
# extract numeric columns
num_train <- train[, -21]

# overall min and max
overall_min <- round(min(num_train),2)
overall_max <- round(max(num_train),2)
minCol_index <- which(num_train == min(num_train), arr.ind = TRUE)[2]
maxCol_index <- which(num_train == max(num_train), arr.ind = TRUE)[2]
minCol_name <- colnames(num_train)[minCol_index]
maxCol_name <- colnames(num_train)[maxCol_index]

# ranges
ranges <- apply(num_train, 2, function(x) diff(range(x)))
maxRange <- round(max(ranges),2)
minRange <- round(min(ranges),2)
maxRange_name <- colnames(num_train)[which.max(ranges)]
minRange_name <- colnames(num_train)[which.min(ranges)]

# standard deviations
sd_vals <- apply(num_train, 2, sd)
maxSD <- round(max(sd_vals),2)
minSD <- round(min(sd_vals),2)
maxSD_name <- colnames(num_train)[which.max(sd_vals)]
minSD_name <- colnames(num_train)[which.min(sd_vals)]

# means
mean_vals <- apply(num_train, 2, mean)
maxMean <- round(max(mean_vals),2)
minMean <- round(min(mean_vals),2)
maxMean_name <- colnames(num_train)[which.max(mean_vals)]
minMean_name <- colnames(num_train)[which.min(mean_vals)]

# variances
var_vals <- apply(num_train, 2, var)
maxVar <- round(max(var_vals),2)
minVar <- round(min(var_vals),2)
maxVar_name <- colnames(num_train)[which.max(var_vals)]
minVar_name <- colnames(num_train)[which.min(var_vals)]


```

`train` has `r nrow(train)` rows, with an overall minimum value of `r overall_min` (`r minCol_name`) and an overal maximum value of `r overall_max` (`r maxCol_name`).

The variables have different scales and variances:

<!-- | Statistic | Min                             | Max                             | -->

<!-- |----------------------------------|-------------------|-------------------| -->

<!-- | Mean      | `r minMean` (`r minMean_name`)   | `r maxMean` (`r maxMean_name`)   | -->

<!-- | Variance  | `r minVar` (`r minVar_name`)     | `r maxVar` (`r maxVar_name`)     | -->

<!-- | Range     | `r minRange` (`r minRange_name`) | `r maxRange` (`r maxRange_name`) | -->

<!-- : Table 4: Statistics across Variables -->

```{r minmax_statistics, include=TRUE, echo=FALSE, results='asis'}
#| label: tbl-minmaxStats
#| tbl-cap: "Minimum / Maximum Statistics in 'train'"
#| tbl-cap-location: bottom
#| tbl-alt: "Table with minimum and maximum mean, variance and range in train dataset"


# data frame with the statistics and their values
statistics <- data.frame(
  Statistic = c("Mean", "Variance", "Range"),
  "Minimum" = c(paste0(minMean, " (", minMean_name, ") "), paste0(minVar, " (", minVar_name, ") "), paste0(minRange, " (", minRange_name, ") ")),
  "Maximum" = c(paste0(maxMean, " (", maxMean_name, ") "), paste0(maxVar, " (", maxVar_name, ") "), paste0(maxRange, " (", maxRange_name, ") "))
)


statistics |>
  flextable() |> 
  autofit() |> 
  bold(j=1)

```

At a glance, we can see that there is a significant *spread* across the data in the dataset in terms of mean, variance and range.

```{r scale_train}
# load scale_data function
source("scale_data.R")

# scale num_train
num_train_scaled <- scale_data(num_train)
#dim(num_train_scaled)
#describe(num_train_scaled)
summary(num_train_scaled)

# add label to scaled data
num_train_scaled <- as.data.frame(scale(num_train))
num_train_scaled_label <- cbind(num_train_scaled, label = train$label)

num_train_scaled_label

#save to csv
write.csv(num_train_scaled_label, "../data/num_train_scaled_label.csv", row.names = FALSE)


```

#### Outliers {#sec-outliers}

In addition to variables having different scales, we also need to consider `outliers` - that is, values which are *significantly* different from other data points for a given variable. These could be as a result of error (measurement, data entry), faulty or poorly calibrated equipment, or they could be *genuine* measurements - understanding which is true is essential.

Genuine extreme values need to be retained in order to add valuable information to the model. However, problematic outliers need to be identified and either *treated* or removed before further analysis, as they could skew the analysis and model.

Outliers can be explored visually through plots such as histograms and violinplots. The majority of variables appear to be *normally* distributed as in @fig-distros below, where the distributions per label uniformly overlap. When looking at the distributions grouped by label, there does not appear to be anything significantly concerning.

::: callout-note
Complete plots for all variables are available in the code comments.
:::

```{r ggdensity}
X1<-ggplot(train, aes(x = X1, fill = label)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ label, nrow = 3)
X1 + ggplot(train, aes(x=X7, fill = label)) + 
  geom_density(alpha = 0.5) 
```

```{r ggplot_grid}

# list of plots, with one plot for each variable
plot_list <- lapply(names(train), function(var) {
  ggplot(train, aes_string(x = var, fill = "label")) +
    geom_density(alpha = 0.5) +
    ggtitle(paste0("Distribution of ", var))
})

# Combine the plots into a grid using the cowplot package
plot_grid(plotlist = plot_list, ncol = 5)

```

```{r colours}



#RColorBrewer::display.brewer.all()
brewer.pal(8, "Dark2")
palette <- brewer.pal(5, "Dark2")


#palette <- c("#FFE2CC", "#FFD1B3", "#FFC299", "#B3C6CC", "#99B3CC", "#A3C4BC")
palette2 <- c( "#1B9E77", "C", "#7570B3", "#E7298A", "#66A61E")
palette3 <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#D55E00")
  "#1B9E77" "#D95F02" "#7570B3" "#E7298A" "#66A61E" "#E6AB02" "#A6761D" "#666666"

```

```{r distributions, include=TRUE, echo=FALSE, message = FALSE, tab.align='left', fig.alt="Distributions by 'label'"}
#| label: fig-distros
#| fig-cap: "Distributions by 'label'"}

# list of plots, with one plot for each variable, remove legend
plot_list <- lapply(names(train[,-21]), function(var) {
  ggplot(train, aes_string(x = var, colour = "label", fill = "label", alpha = 0.2)) +
    scale_fill_manual(values = palette2) +
    scale_colour_manual(values = palette2) +
    geom_density(aes(y = after_stat(density)), alpha = 0.2)  +
    ylab(NULL) + 
    theme(legend.position="none")
})

# Combine the plots into a grid using the cowplot package
grid <- plot_grid(plotlist = plot_list, ncol = 5)

# add title 
title <- ggdraw() + 
  draw_label("Distribution of variables", fontface = "bold", size = 20)

# combine with plot_grid function from cowplot
plot_grid(title, grid, ncol = 1, rel_heights = c(0.1, 1))



```

<!--# When looking at histograms, I explored how to best determine which binwidth to use.  I came across Freedman-Diaconis and Sturges rules. The results for `num_train` are below and they are quite different.  For example, X7 - FD has a bw of 0.07 and Sturges has a bw of 0.14, that is, double.  In general, Sturges bw is much large. The reason is that FD takes into account the variability of the data, not just the count of observations (Sturges).  I am going to use FDR. -->

```{r binwidths}


# binwidths using Freedman-Diaconis rule
binwidth_FD <- lapply(num_train, function(x) {
  bw <- 2 * IQR(x) / (length(x)^(1/3))
  round(bw, 2)
})

# binwidths using Sturges rule
binwidth_Stu <- lapply(num_train, function(x) {
  bw <- diff(range(x)) / (1 + log2(length(x)))
  round(bw, 2)
})

# combine binwidths into one table
binwidth_table <- cbind(variable = names(num_train), 
                        binwidth_FD = unlist(binwidth_FD), 
                        binwidth_Stu = unlist(binwidth_Stu))

# print the table
print(binwidth_table)



```

```{r histograms, include=TRUE, echo=FALSE, message = FALSE, tab.align='left', fig.alt="Histograms and Density Plots of Selected Variables"}
#| label: fig-hist
#| fig-cap: "Histograms and Density Plots of Selected Variables in `train`"

# selected variables
var_subset <- num_train[c("X1", "X7", "X8", "X9")]

# binwidth function using Freedman-Diaconis Rule
binwidth_FD <- lapply(var_subset, function(x) {
  bw <- 2 * IQR(x) / (length(x)^(1/3))
  round(bw, 2)
})

# set size, small
theme_set(theme_bw(base_size = 10))

# list of histograms for each variable
histograms <- lapply(names(var_subset), function(x) {
  ggplot(data = data.frame(y = num_train[[x]]), aes(x = y)) +
    geom_histogram(aes(y = after_stat(density)), fill = "#A3C4BC", color = "#616161", alpha = 0.6,
                   binwidth = binwidth_FD[[x]]) +
    geom_density(aes(y = after_stat(density)), alpha = 0.6) +
    labs(x = x, y = NULL) +
    theme(axis.text.x = element_blank(), axis.title.y = element_text(margin = margin(r = 10)))
})

# histograms with cowplot package
hists <- plot_grid(plotlist = histograms, ncol = 2, align = "v") +
  ggtitle("Histograms and Density Plots of Selected Variables") +
  theme(plot.title = element_text(hjust = 0.5, size = 14),
        plot.margin = unit(c(1, 1, 1, 1), "cm"))

hists

```

The above variables were chosen to illustrate varieties in shape and distribution within the sample data. In the distribution and density plots (@fig-hist), we see that some variables have non-normal distributions, with multiple peaks or skewness. The violin plots below (@fig-violin) include a boxplot where we can more easily see the presence of outliers. The box contains data within the `interquartile range` (IQR), which represents 50% of the data; the points beyond the *whisker* are data beyond 1.5x the IQR and are often considered 'outliers'. In the subset below, X8 has many more potential outliers than variable X9.

```{r violins, include=TRUE, echo=FALSE, message = FALSE, fig.alt="Violin Plots of Selected Variables"}
#| label: fig-violin
#| fig-cap: "Violin Plots of Selected Variables in `train`"

# variables of interest
#var_subset <- num_train[c("X1", "X7", "X8", "X9")]

# binwidths for histograms
#binwidths <- lapply(var_subset, function(x) diff(range(x))/30)

# list of labeled boxplot and violin plot for each variable
viol_box <- lapply(names(var_subset), function(x) {

  # # create a labeled boxplot
  # boxplot <- ggplot(data = data.frame(y = var_subset[[x]]), aes(y = y)) +
  #   geom_boxplot(fill = "#A3C4BC", color = "#616161") +
  #   ggtitle(paste("Boxplot of", x)) +
  #   xlab(x) +
  #   ylab("Value") +
  #   theme_bw(base_size = 10) +
  #   theme(plot.title = element_text(hjust = 0.5, size = 14),
  #         axis.text.x = element_blank(),
  #         axis.title.y = element_text(margin = margin(r = 10)))

  # violin plot
  violinplot <- ggplot(data = data.frame(y = var_subset[[x]]), aes(x = "", y = y)) +
    geom_violin(fill = "#A3C4BC", color = "#616161", alpha = 0.6) +
    geom_boxplot(width = 0.2, fill = "#7570B3", color = "#616161", alpha = 0.6) +
    #ggtitle(paste("Violin Plot of", x)) +
    xlab(x) +
    ylab("") +
    theme_bw(base_size = 10) +
    theme(plot.title = element_text(hjust = 0.5, size = 14),
          axis.text.x = element_blank(),
          axis.title.y = element_text(margin = margin(r = 10)))

  # return list with boxplot and violin plot
  list(
    #boxplot, 
    violinplot)
})

# flatten the list of plots, arrange 
plots_flat <- unlist(viol_box, recursive = FALSE)

violins <- plot_grid(plotlist = plots_flat, 
          #nrow = 2, 
          ncol = 4) +
  ggtitle("Violin Plots of Selected Variables") +
  theme(plot.title = element_text(hjust = 0.5, size = 14),
        plot.margin = unit(c(1, 1, 1, 1), "cm"))

violins
```

<!--# Below are the histograms and boxplots for all variables. -->

```{r train_histograms_all}

# selected variables
var_subset <- num_train

# binwidth function using Freedman-Diaconis Rule
binwidth_FD <- lapply(var_subset, function(x) {
  bw <- 2 * IQR(x) / (length(x)^(1/3))
  round(bw, 2)
})

# set size, small
theme_set(theme_bw(base_size = 10))

# list of histograms for each variable
histograms_all <- lapply(names(var_subset), function(x) {
  ggplot(data = data.frame(y = num_train[[x]]), aes(x = y)) +
    geom_histogram(aes(y = after_stat(density)), fill = "#A3C4BC", color = "#616161",
                   binwidth = binwidth_FD[[x]]) +
    geom_density(aes(y = after_stat(density)), alpha = 0.8) +
    labs(x = x, y = NULL) +
    theme(axis.text.x = element_blank(), axis.title.y = element_text(margin = margin(r = 10)))
})

# histograms with cowplot package
hists_all <- plot_grid(plotlist = histograms_all, ncol = 4, align = "v") +
  ggtitle("Histograms and Density Plots of Selected Variables") +
  theme(plot.title = element_text(hjust = 0.5, size = 14),
        plot.margin = unit(c(1, 1, 1, 1), "cm"))

hists_all


```

```{r train_boxplots_all}
# set size, small
theme_set(theme_bw(base_size = 10))

# list of boxplots for each variable
boxplots <- lapply(num_train, function(x) ggplot(data = data.frame(y = x), aes(y = y)) +
               geom_boxplot(fill = "#A3C4BC", color = "#616161") +
               labs(x = NULL, y = NULL) +
               coord_fixed(ratio = .04) +
               theme(axis.text.x = element_blank(), axis.title.y = element_text(margin = margin(r = 10)))) 

# boxplots with cowplot package
plot_grid(plotlist = boxplots, ncol = 4, align = "h") +
  ggtitle("Boxplots of Variables in `num_train`") +
  theme(plot.title = element_text(hjust = 0.5, size = 14),
        plot.margin = unit(c(1, 1, 1, 1), "cm"))
```

<!--# To be certain, I have reproduced the above plots on the scaled data.  The density plots remain the same but the binwidths are different, producing a slightly different visual.  The bins for X7-X9 are clearly larger for the scaled dataset, indicating that there is a smaller range of values in comparison.  However, they are the same shape.-->

```{r hists_sc}


# selected variables
var_subset <- num_train_scaled

# binwidth function using Freedman-Diaconis Rule
binwidth_FD <- lapply(var_subset, function(x) {
  bw <- 2 * IQR(x) / (length(x)^(1/3))
  round(bw, 2)
})

# set size, small
theme_set(theme_bw(base_size = 10))

# list of histograms for each variable
histograms_sc <- lapply(names(var_subset), function(x) {
  ggplot(data = data.frame(y = num_train[[x]]), aes(x = y)) +
    geom_histogram(aes(y = after_stat(density)), fill = "#A3C4BC", color = "#616161",
                   binwidth = binwidth_FD[[x]]) +
    geom_density(aes(y = after_stat(density)), alpha = 0.8) +
    labs(x = x, y = NULL) +
    theme(axis.text.x = element_blank(), axis.title.y = element_text(margin = margin(r = 10)))
})

# histograms with cowplot package
hists_sc <- plot_grid(plotlist = histograms_sc, ncol = 4, align = "v") +
  ggtitle("Histograms and Density Plots of Selected Variables (scaled)") +
  theme(plot.title = element_text(hjust = 0.5, size = 14),
        plot.margin = unit(c(1, 1, 1, 1), "cm"))

hists_sc
```

```{r box_sc}


# set size, small
theme_set(theme_bw(base_size = 10))

# list of boxplots for each variable
boxplots_scale <- lapply(num_train_scaled, function(x) ggplot(data = data.frame(y = x), aes(y = y)) +
               geom_boxplot(fill = "#A3C4BC", color = "#616161") +
               labs(x = NULL, y = NULL) +
               theme(axis.text.x = element_blank(), axis.title.y = element_text(margin = margin(r = 10)))) 

# boxplots with cowplot package
plot_grid(plotlist = boxplots_scale, ncol = 5, align = "h") +
  ggtitle("Boxplots of Variables in `num_train_scaled`") +
  theme(plot.title = element_text(hjust = 0.5, size = 16),
        plot.margin = unit(c(1, 1, 1, 1), "cm"))
```

<!--# In addition to visually inspecting the variables in terms of distributions and densities, I looked at a more formal approach to identify outliers in the dataset.  I calculated z-scores for each variable using 'scale()'.  -->

<!--# Then outliers where pulled out - those with a z-score of +/- 3 which is a common threshold.  In a normal distribution - 99.7% of observations fall within 3 standard deviations. -->

Another approach for exploring outliers is to calculate a statistic (*z-score*) for each variable and extract those which exceed the threshold (+/- 3 standard deviations). In a normal distribution, 99.7% of observations fall within 3 standard deviation, so we identify 0.03% of observations either side of the centre of the variable and consider them as *potential* outliers.

```{r outliers_zscore}
# calculate z-scores for each variable
z_scores <- apply(num_train, 2, function(x) scale(x, center = TRUE, scale = TRUE))

# identify values with z-score greater than 3 or less than -3
outliers <- which(abs(z_scores) > 3, arr.ind = TRUE)

# new df with column name, number 
outliers_df <- data.frame(
  col_name = colnames(num_train)[outliers[, "col"]],
  col_num = outliers[, "col"],
  row_num = outliers[, "row"]
)

nrow(outliers_df)


# get table of outliers by variable
table_outliers <- table(outliers_df$col_name)

#flextable(as.data.frame(table_outliers))
plot(table_outliers)

```

<!--# I used two different methods to check for outliers within each variable - the first was using z-scores and values which are either side of 3 standard deviations.  This resulted in 81 outliers, with X8 having the most.  A more robust method is using mean absolute deviation which can be seen below.  This resulted in 119 potential outliers.  The interesting thing is that X8 now has 38 potential outliers.  It could be that X8 has more extreme variables or has more variability - but the client should be made aware.   -->

```{r outliers_mad}

# function to return median absolute deviation (MAD) outliers, default = 3

outliers_mad <- function(df, N =3) {
  # initialise dataframe to store outliers
  outliers <- data.frame()
  
  # loop through each column 
  for (col in names(df)) {
    # check if numeric
    if (is.numeric(df[[col]])) {
      # calculate MAD for the column
      mad <- mad(df[[col]])
      
      # identify outliers using MAD and N
      outliers <- rbind(outliers, data.frame(column = col, value = df[[col]])[abs(df[[col]] - median(df[[col]])) > N * mad, ])
    }
  }
  
  # return dataframe of outliers
  return(outliers)
}

# store outlier results
mads <- outliers_mad(num_train, 3)

# function to produce count flextable
outliers_FT <- function(outliers, cap = "") {
  # count number of outliers per variable
  counts <- table(outliers$column)
  # data frame with 'Variable' and 'Count'
  result_df <- data.frame(Variable = names(counts), Count = as.numeric(counts))
  # sort by count
  result_df <- arrange(result_df, desc(Count))
  # flextable
  FT <- flextable(result_df) |>
    set_caption(cap) 
  
  return(FT)
}
nrow(mads)
mad_outlier <- outliers_FT(outliers = mads, cap = "Counts of outliers by Variable")
mad_outlier
```

```{r mahalanobis}

# function to return multidimensional outliers with mahalanobis distance, default = 3

outliers_mahalanobis <- function(df, N = 3) {
  # Check for outliers using the Mahalanobis distance for all columns in the dataframe.
  # Calculate the covariance matrix of the dataframe
  cov <- cov(df)
  # Calculate the Mahalanobis distance for each observation
  distances <- mahalanobis(df, center = colMeans(df), cov = cov)
  # Identify outliers
  outliers <- df[distances > N * sqrt(diag(cov)), ]
  return(outliers)
}





# store outlier results
maha <- outliers_mahalanobis(num_train, 150)

# function to produce count flextable
outliers_FT <- function(outliers, cap = "") {
  # count number of outliers per variable
  counts <- table(outliers$column)
  # data frame with 'Variable' and 'Count'
  result_df <- data.frame(Variable = names(counts), Count = as.numeric(counts))
  # sort by count
  result_df <- arrange(result_df, desc(Count))
  # flextable
  FT <- flextable(result_df) |>
    set_caption(cap)

  return(FT)
}

nrow(maha)
maha_outlier <- outliers_FT(maha)
maha_outlier

```

```{r}
library(assertr)

maha <- maha_dist(num_train)
#length(maha)

maha_dist(num_train) %>% hist(main="", xlab="")




# Calculate the Mahalanobis distance for each row of the dataset
maha <- mahalanobis(num_train, center = colMeans(num_train), cov = cov(num_train))

# Identify the rows that are more than N MADs away from the center
violations <- which(maha > 3 * mad(maha))

# Save the violations to a data frame
violations_df <- data.frame(Violation = violations, num_train[violations, ])
nrow(violations)
#flextable(violations)

violations2 <- num_train %>%
  insist_rows(maha_dist, within_n_mads(3), dplyr::everything()) %>%
  as.data.frame()

```

```{r}
library(assertr)

# define your custom assertion function
maha_dist_within_n_mads <- function(df, n_mads) {
  # compute the Mahalanobis distance for each row of the data frame
  mahadist <- mahalanobis(df, colMeans(df), cov(df))
  # calculate the number of MADs each row is from the mean Mahalanobis distance
  nmad <- mad(mahadist)
  # return a logical vector indicating whether each row is within the specified number of MADs
  return(nmad <= n_mads)
}

# apply the assertion function and capture the violations as a data frame
violations <- num_train %>%
  assert_rows(maha_dist_within_n_mads, n_mads = 3, error_fun = error_handler_defect_df_return())

# print the violations
print(violations)


```

```{r}
#Set the error_fun parameter to error_append
my_defect_df_return <- function(data) {
  function(errors) {
    if (length(errors) == 0) {
      return(data)
    } else {
      skipped_rules <- do.call(rbind, lapply(errors, function(e) {
        data.frame(predicate = e$predicate, column = e$column, index = e$index, value = e$value)
      }))
      return(skipped_rules)
    }
  }
}

violations <- num_train %>%
  insist(maha_dist, within_n_mads(3), dplyr::everything(),
         error_fun = my_defect_df_return(.), rule_name = "insist_rows")



#Print the violations dataframe
violations

```

```{r maha_outliers}
find_outliers_mahalanobis <- function(df, N) {
  # Calculate the Mahalanobis distance for each observation
  distances <- mahalanobis(x = df, center = colMeans(df), cov = cov(df))
  
  # Set the cutoff value for distances
  cutoff <- qchisq(p = 0.95, df = ncol(df))
  
  # Identify the observations with distances greater than the cutoff
  outliers <- df[distances > N * sqrt(cutoff), ]
  
  return(outliers)
}



# Find outliers using a cutoff value of 3
outliers <- find_outliers_mahalanobis(num_train, 7)
nrow(outliers)

```

```{r}
mahalanobis_outlier <- function(data, alpha = 0.05) {
  center <- apply(data, 2, mean)
  cov <- cov(data)
  distances <- mahalanobis(data, center, cov)
  cutoff <- qchisq(1-alpha, df = ncol(data))
  outliers <- data[distances > cutoff, ]
  return(outliers)
}

outliers <- mahalanobis_outlier(num_train)
nrow(outliers)

```

`r case_when(nrow(table_outliers) == 0 ~ "There are no 'outliers' in the dataset.", nrow(table_outliers) == 1 ~ { max_outliers <- max(table_outliers) col_max_outliers <- names(table_outliers)[which.max(table_outliers)] paste0("There is ", nrow(outliers_df), " 'outlier' in the dataset from ", col_max_outliers, ".")   },nrow(table_outliers) == ncol(num_train) ~ {   max_outliers <- max(table_outliers)   col_max_outliers <- names(table_outliers)[which.max(table_outliers)]   paste0("There are a total of ", nrow(outliers_df), " 'outliers' in the dataset. Every variable in the dataset has at least one 'outlier' with variable '", col_max_outliers, "' having the most with ", max_outliers, ".")   }, nrow(table_outliers) < nrow(num_train) ~ {     max_outliers <- max(table_outliers)   col_max_outliers <- names(table_outliers)[which.max(table_outliers)]   paste0("There are a total of ", nrow(outliers_df), " 'outliers' in the dataset from ", nrow(table_outliers), " variables, with variable '", col_max_outliers, "' having the most with ", max_outliers, ".")})`

```{r outlier_sentence}

# check if there are 0 outliers
if (nrow(table_outliers) == 0) {
  print("There are no 'outliers' in the dataset.")
# check if there is 1 outlier
} else if (nrow(table_outliers) ==1) {
  max_outliers <- max(table_outliers)
  col_max_outliers <- names(table_outliers)[which.max(table_outliers)]
  print(paste0("There is ", nrow(outliers_df), " 'outlier' in the dataset from ", col_max_outliers, "."))
# check if variables with outliers ==  variables in num_train
} else if (nrow(table_outliers) == ncol(num_train)) {
  max_outliers <- max(table_outliers)
  col_max_outliers <- names(table_outliers)[which.max(table_outliers)]
  print(paste0("There are a total of ", nrow(outliers_df), " 'outliers' in the dataset. Every variable in the dataset has at least one 'outlier' with variable '", col_max_outliers, "' having the most with ", max_outliers, "."))
# check if less variables with outliers than vars in num_train
} else if (nrow(table_outliers) < nrow(num_train)) {
  max_outliers <- max(table_outliers)
  col_max_outliers <- names(table_outliers)[which.max(table_outliers)]
  print(paste0("There are a total of ", nrow(outliers_df), " 'outliers' in the dataset from ", nrow(table_outliers), " variables, with variable '", col_max_outliers, "' having the most with ", max_outliers, "."))
}


```

::: callout-important
## Decision

Given no prior knowledge about the dataset or domain, the `train` data will be *scaled* before proceeding, resulting in a dataset where all variables have a mean of 0 and a standard deviation of 1.

This mitigates against any single variable having undue influence in the classification model as a result of different scales.

Outliers will be retained. In the absence of anything to the contrary, it is assumed that they are genuine measurements which will add to the model's performance.
:::

::: callout-caution
With their domain knowledge, the client should consider whether outliers are problematic. Additional contextual information could result in different approaches to handling outliers, including removal or transformation.
:::

### Correlation between Variables {#sec-correlation-between-variables}

```{r pairspanel}
# uncomment to run a not-so-useful pairs plot

#par(mar = c(1, 1, 1, 1))
#pairs.panels(train)

```

```{r corrmatrix, include=TRUE, echo=FALSE, message = FALSE, fig.alt="Correlation Matrix"}
#| label: fig-corrmatrix
#| fig-cap: "Correlation Matrix"

# create correlation matrix
corr_matrix <- cor(num_train)

# plot correlation matrix using corrplot
corrplot(corr_matrix, method = 'ellipse', type = 'lower', insig='blank',
           diag=FALSE, tl.col = 'grey30')

```

Having explored the [overall statistics](#sec-overall-statistics) and [outliers](#sec-outliers) above, this section briefly describes linear relationships between variables. The `correlation matrix` above shows that many variables are not linearly correlated - indicated by blank squares, or faint colour. However, there are also some strong correlations such as the strong negative correlation between X18, X20 each with X17, X19; while there is strong positive correlation between X18 and X20 as well as between X17 and X19.

Looking at these variables more closely in the `pair plots` below, we can see the strong linear relationships - the ellipsoid shapes indicating that as the value of one variable goes up, the corresponding value in the other variable goes down (negative relationship) or up (positive relationship). The accompanying coefficients (ranging from -1 (perfect negative correlation) to 1 (perfect positive correlation) indicate the strength of the relationships.

::: callout-note
Additional plots are available in the code comments.
:::

```{r ggcorrplot}
# alternative corrplots

# create correlation matrix
ggcorr <- round(cor(num_train), 1)

# p values
p.mat <- cor_pmat(num_train)

# plot correlation matrix using ggcorrplot
ggcorrplot(ggcorr, hc.order = TRUE, type = "lower")

ggcorrplot(ggcorr,
  hc.order = TRUE, type = "full", p.mat = p.mat,insig = "blank",
  outline.color = "white",
  ggtheme = ggplot2::theme_gray,
  colors = c("#6D9EC1", "white", "#E46726")
)
```

```{r ggpairs}

#pair plots broken into sets of 5

pair1_5 <- train %>%
  select(label, X1, X2, X3, X4, X5) %>% 
  GGally::ggpairs(aes(color = label),
          columns = c("X1", "X2", "X3", "X4", "X5"), progress = FALSE) 

pair6_10 <- train %>%
  select(label, X6, X7, X8, X9, X10) %>% 
  GGally::ggpairs(aes(color = label),
          columns = c("X6","X7", "X8", "X9","X10"), progress = FALSE) 

pair11_15 <- train %>%
  select(label, X11, X12, X13, X14, X15, X16, X17, X18) %>% 
  GGally::ggpairs(aes(color = label),
          columns = c("X11","X12","X13", "X14", "X15"), progress = FALSE)

pair16_20 <- train %>%
  select(label, X16, X17, X18, X19, X20) %>% 
  GGally::ggpairs(aes(color = label),
          columns = c("X16", "X17", "X18", "X19", "X20"), progress = FALSE)

pair1_5
pair6_10
pair11_15
pair16_20

```

```{r ggpair_16_20, include=TRUE, echo=FALSE, message = FALSE, fig.alt="Correlation Pair Plots - Variables X16-X20"}
#| label: fig-corrmatrix
#| fig-cap: "Correlation Pair Plots - Variables X16-X20"

pair16_20
```

The next section will consider two approaches:

-   dimension reduction - where the information in the variables is distilled into fewer 'dimensions'
-   feature reduction - whether any variables can be excluded from the classification

### 
