---
title: "Chemical Sample Classification Report"
#subtitle: "Exploratory Data"
author: "Petter LÃ¶vehagen"
date: "08 May 2023"

RVersion: 3.6.0 
RStudio: 2023.03.0+386 Cherry Blossom Release
Platform: "x86_64-w64-mingw32/x64 (64-bit)"
OS: "Windows 11 SE"

format: 
  docx: default
  html:
    toc: true
    toc-depth: 3
    toc-title: "Contents"
    toc-location: left
    code-fold: true
    code-line-numbers: true
    theme: spacelab
    html-math-method: mathjax
    highlight: "tango"
    #css: "../data/custom.css"
output:
  word_document:
    fig.retina: 300
prefer-docx: true
lang: "en-GB"
bibliography: "../data/AS_CW_References.bib"
csl: "../data/harvard-university-of-the-west-of-england.csl"

knitr:
  opts_chunk:
    include: false
    tidy: true
    echo: false
    warning: false
    message: false
    error: true
    out.width: '90%'
    #fig.width: 6
    #fig.height: 4
---

<!--# The 'chem_packages.R file installs/loads required packages for this section, sets a seed (for reproducibility) and loads flextable defaults. Check for further details. -->

```{r EDA_packages}
# load/install R packages/libraries
# set seed
# load flextable defaults

file <- "EDA"
source("chem_packages.R")


# what is currently loaded/attached:
#sessionInfo()

# for this section
# library(flextable)
# library(dplyr)
# library(ggplot2)
# library(cowplot)
# library(corrplot)
# library(GGally)
# library(psych)
# library(patchwork)

```

```{r load_data_from_csv}
# load train data set
train <-read.csv("../data/chem_train.csv")
# extract numeric columns
num_train <- train[, -21]

```

```{r train_str}
# structure and summary

head(train)
dim(train)
str(train)
summary(train)
```

```{r stats_psych}
#summary of stats using 'psych'

describe(train)
```

<!--# Looking at the overall statistics of the dataset before any transformation, there are different scales between the variables.  Without knowing the units of measurement or anything else, the data will be mean scaled/normalised to ensure that variables are on the same scale. -->

### Exploratory Data Analysis {#sec-exploratory-data-analysis}

Only the *train* dataset will be analysed in the EDA stage.

#### Overall Size and Shape {#sec-overall-statistics}

```{r trainstatistics}
# extract numeric columns
num_train <- train[, -21]

# overall min and max
overall_min <- round(min(num_train),2)
overall_max <- round(max(num_train),2)
minCol_index <- which(num_train == min(num_train), arr.ind = TRUE)[2]
maxCol_index <- which(num_train == max(num_train), arr.ind = TRUE)[2]
minCol_name <- colnames(num_train)[minCol_index]
maxCol_name <- colnames(num_train)[maxCol_index]

# ranges
ranges <- apply(num_train, 2, function(x) diff(range(x)))
maxRange <- round(max(ranges),2)
minRange <- round(min(ranges),2)
maxRange_name <- colnames(num_train)[which.max(ranges)]
minRange_name <- colnames(num_train)[which.min(ranges)]

# standard deviations
sd_vals <- apply(num_train, 2, sd)
maxSD <- round(max(sd_vals),2)
minSD <- round(min(sd_vals),2)
maxSD_name <- colnames(num_train)[which.max(sd_vals)]
minSD_name <- colnames(num_train)[which.min(sd_vals)]

# means
mean_vals <- apply(num_train, 2, mean)
maxMean <- round(max(mean_vals),2)
minMean <- round(min(mean_vals),2)
maxMean_name <- colnames(num_train)[which.max(mean_vals)]
minMean_name <- colnames(num_train)[which.min(mean_vals)]

# variances
var_vals <- apply(num_train, 2, var)
maxVar <- round(max(var_vals),2)
minVar <- round(min(var_vals),2)
maxVar_name <- colnames(num_train)[which.max(var_vals)]
minVar_name <- colnames(num_train)[which.min(var_vals)]


```

`train` has `r nrow(train)` rows, with an overall minimum value of `r overall_min` (`r minCol_name`) and an overal maximum value of `r overall_max` (`r maxCol_name`).

The variables have different scales and variances:

<!-- | Statistic | Min                             | Max                             | -->

<!-- |----------------------------------|-------------------|-------------------| -->

<!-- | Mean      | `r minMean` (`r minMean_name`)   | `r maxMean` (`r maxMean_name`)   | -->

<!-- | Variance  | `r minVar` (`r minVar_name`)     | `r maxVar` (`r maxVar_name`)     | -->

<!-- | Range     | `r minRange` (`r minRange_name`) | `r maxRange` (`r maxRange_name`) | -->

<!-- : Table 4: Statistics across Variables -->

```{r minmaxStats, include=TRUE, echo=FALSE, results='asis'}
#| label: tbl-minmaxStats
#| tbl-cap: "Minimum / Maximum Statistics in 'train'"
#| tbl-cap-location: bottom
#| tbl-alt: "Table with minimum and maximum mean, variance and range in train dataset"


# data frame with the statistics and their values
statistics <- data.frame(
  Statistic = c("Mean", "Variance", "Range"),
  "Minimum" = c(paste0(minMean, " (", minMean_name, ") "), paste0(minVar, " (", minVar_name, ") "), paste0(minRange, " (", minRange_name, ") ")),
  "Maximum" = c(paste0(maxMean, " (", maxMean_name, ") "), paste0(maxVar, " (", maxVar_name, ") "), paste0(maxRange, " (", maxRange_name, ") "))
)


statistics |>
  flextable() |> 
  autofit() |> 
  bold(j=1)

```

At a glance, we can see that there is a significant *spread* across the data in the dataset in terms of mean, variance and range.

```{r scale_train}
# load scale_data function
source("scale_data.R")

# scale num_train
num_train_scaled <- scale_data(num_train)
#dim(num_train_scaled)
#describe(num_train_scaled)
summary(num_train_scaled)

# add label to scaled data
num_train_scaled <- as.data.frame(scale(num_train))
num_train_scaled_label <- cbind(num_train_scaled, label = train$label)

num_train_scaled_label

#save to csv
write.csv(train, "../data/num_train_scaled_label.csv", row.names = FALSE)


```

#### Outliers

In addition to variables having different scales, we also need to consider `outliers` - that is, values which are *significantly* different from other data points for a given variable. These could be as a result of error (measurement, data entry), faulty or poorly calibrated equipment, or they could be *genuine* measurements.

Genuine extreme values need to be retained in order to add valuable information to the model. However, problematic outliers need to be identified and either *treated* or removed before further analysis, as they could skew the analysis and model.

Outliers can be explored visually through plots such as histograms and violinplots. The majority of variables appear to be *normally* distributed, but there are some non-normal distributions. The below variables were chosen to illustrate the variety in shape and distribution within the sample data, although most variables have a similar shape to 'X1'.

```{r hists, include=TRUE, echo=FALSE, message = FALSE, tab.align='left', fig.alt="Histograms and Density Plots of Selected Variables"}

#| label: fig-hist
#| fig-cap: "Histograms and Density Plots of Selected Variables in `train`"

# selected variables
var_subset <- num_train[c("X1", "X7", "X8", "X9")]

# binwidth function using Freedman-Diaconis Rule
binwidths <- lapply(var_subset, function(x) diff(range(x))/30)

# set size, small
theme_set(theme_bw(base_size = 10))

# list of histograms for each variable
histograms <- lapply(names(var_subset), function(x) {
  ggplot(data = data.frame(y = num_train[[x]]), aes(x = y)) +
    geom_histogram(aes(y = after_stat(density)), fill = "#A3C4BC", color = "#616161",
                   binwidth = binwidths[[x]]) +
    geom_density(aes(y = after_stat(density)), alpha = 0.8) +
    labs(x = x, y = NULL) +
    theme(axis.text.x = element_blank(), axis.title.y = element_text(margin = margin(r = 10)))
})

# histograms with cowplot package
hists <- plot_grid(plotlist = histograms, ncol = 2, align = "v") +
  ggtitle("Histograms and Density Plots of Selected Variables") +
  theme(plot.title = element_text(hjust = 0.5, size = 14),
        plot.margin = unit(c(1, 1, 1, 1), "cm"))

hists



```

In the distribution and density plots (@fig-hist), we see that some variables have non-normal distributions, with several peaks or skewness. The violin plots below (@fig-violin) include a boxplot where we can more easily see the outliers in the variables. They are represented by the dots on the tails of the violins. Every variable

```{r hists_sc}

#| label: fig-hist
#| fig-cap: "Histograms and Density Plots of Selected Variables in `train`"

# selected variables
var_subset_sc <- num_train_scaled[c("X1", "X7", "X8", "X9")]

# binwidth function using Freedman-Diaconis Rule
binwidths <- lapply(var_subset, function(x) diff(range(x))/30)

# set size, small
theme_set(theme_bw(base_size = 10))

# list of histograms for each variable
histograms <- lapply(names(var_subset), function(x) {
  ggplot(data = data.frame(y = num_train[[x]]), aes(x = y)) +
    geom_histogram(aes(y = after_stat(density)), fill = "#A3C4BC", color = "#616161",
                   binwidth = binwidths[[x]]) +
    geom_density(aes(y = after_stat(density)), alpha = 0.8) +
    labs(x = x, y = NULL) +
    theme(axis.text.x = element_blank(), axis.title.y = element_text(margin = margin(r = 10)))
})

# histograms with cowplot package
hists_sc <- plot_grid(plotlist = histograms, ncol = 2, align = "v") +
  ggtitle("Histograms and Density Plots of Selected Variables") +
  theme(plot.title = element_text(hjust = 0.5, size = 14),
        plot.margin = unit(c(1, 1, 1, 1), "cm"))

hists_sc
```

```{r violins, include=TRUE, echo=FALSE, message = FALSE, tab.align='left', fig.alt="Violin Plots of Selected Variables"}
#| label: fig-violin
#| fig-cap: "Violin Plots of Selected Variables in `train`"

# variables of interest
#var_subset <- num_train[c("X1", "X7", "X8", "X9")]

# binwidths for histograms
#binwidths <- lapply(var_subset, function(x) diff(range(x))/30)

# list of labeled boxplot and violin plot for each variable
viol_box <- lapply(names(var_subset), function(x) {

  # # Create a labeled boxplot
  # boxplot <- ggplot(data = data.frame(y = var_subset[[x]]), aes(y = y)) +
  #   geom_boxplot(fill = "#A3C4BC", color = "#616161") +
  #   ggtitle(paste("Boxplot of", x)) +
  #   xlab(x) +
  #   ylab("Value") +
  #   theme_bw(base_size = 10) +
  #   theme(plot.title = element_text(hjust = 0.5, size = 14),
  #         axis.text.x = element_blank(),
  #         axis.title.y = element_text(margin = margin(r = 10)))

  # violin plot
  violinplot <- ggplot(data = data.frame(y = var_subset[[x]]), aes(x = "", y = y)) +
    geom_violin(fill = "#A3C4BC", color = "#616161") +
    geom_boxplot(width = 0.2, fill = "white", color = "#616161") +
    #ggtitle(paste("Violin Plot of", x)) +
    xlab(x) +
    ylab("") +
    theme_bw(base_size = 10) +
    theme(plot.title = element_text(hjust = 0.5, size = 14),
          axis.text.x = element_blank(),
          axis.title.y = element_text(margin = margin(r = 10)))

  # Return a list with labeled boxplot and violin plot
  list(
    #boxplot, 
    violinplot)
})

# Flatten the list of plots and arrange them in a grid
plots_flat <- unlist(viol_box, recursive = FALSE)

violins <- plot_grid(plotlist = plots_flat, 
          #nrow = 2, 
          ncol = 4) +
  ggtitle("Violin Plots of Selected Variables") +
  theme(plot.title = element_text(hjust = 0.5, size = 14),
        plot.margin = unit(c(1, 1, 1, 1), "cm"))

violins

```

```{r train_box}

# set size, small
theme_set(theme_bw(base_size = 10))

# list of boxplots for each variable
boxplots <- lapply(num_train, function(x) ggplot(data = data.frame(y = x), aes(y = y)) +
               geom_boxplot(fill = "#A3C4BC", color = "#616161") +
               labs(x = NULL, y = NULL) +
               coord_fixed(ratio = .04) +
               theme(axis.text.x = element_blank(), axis.title.y = element_text(margin = margin(r = 10)))) 

# boxplots with cowplot package
plot_grid(plotlist = boxplots, ncol = 4, align = "h") +
  ggtitle("Boxplots of Variables in `num_train`") +
  theme(plot.title = element_text(hjust = 0.5, size = 14),
        plot.margin = unit(c(1, 1, 1, 1), "cm"))




```

```{r default_hist_train}



# set size, small
theme_set(theme_bw(base_size = 10))

# list of histograms for each variable
histograms <- lapply(num_train, function(x) ggplot(data = data.frame(y = x), aes(x = y)) +
               geom_histogram(aes(y = after_stat(density)), fill = "#A3C4BC", color = "#616161") +
               geom_density(aes(y = after_stat(density)), alpha = 0.8) +
               labs(x = NULL, y = NULL) +
               theme(axis.text.x = element_blank(), axis.title.y = element_text(margin = margin(r = 10))))

               
# histograms with cowplot package
plot_grid(plotlist = histograms, ncol = 4, align = "h") +
  ggtitle("Histograms and Density Plots of Variables in `num_train`") +
  theme(plot.title = element_text(hjust = 0.5, size = 14),
        plot.margin = unit(c(1, 1, 1, 1), "cm"))
```

Another approach is to calculate a statistic (*z-score*) for each variable.

\[insert explanation and outcome\]

::: callout-important
## Decision

Given no prior knowledge about the dataset or domain, the `train` data will be *scaled* before proceeding, resulting in a dataset where all variables have a mean of 0 and a standard deviation of 1.

This mitigates against any single variable having undue influence in the classification model as a result of different scales.

Outliers will be retained. It is assumed that they are genuine measurements which will add to the model's performance.
:::

::: callout-caution
With their domain knowledge, the client should consider whether outliers are problematic. Additional contextual information could result in different approaches to handling outliers, including removal or transformation.
:::

<!--# I investigated outliers on the raw data in case they are masked or distorted in the scaling process by plotting boxplots of each variable.  There is no obvious difference between the scaled and non-scaled datasets in terms of structure and outliers.  -->

<!--# The plots below are not suitable for a report but if opened and enlarged are suitable for EDA. -->

```{r scaletrain_box}


# set size, small
theme_set(theme_bw(base_size = 10))

# list of boxplots for each variable
boxplots_scale <- lapply(num_train_scaled, function(x) ggplot(data = data.frame(y = x), aes(y = y)) +
               geom_boxplot(fill = "#A3C4BC", color = "#616161") +
               labs(x = NULL, y = NULL) +
               theme(axis.text.x = element_blank(), axis.title.y = element_text(margin = margin(r = 10)))) 

# boxplots with cowplot package
plot_grid(plotlist = boxplots_scale, ncol = 5, align = "h") +
  ggtitle("Boxplots of Variables in `num_train_scaled`") +
  theme(plot.title = element_text(hjust = 0.5, size = 16),
        plot.margin = unit(c(1, 1, 1, 1), "cm"))


```

<!--#I plotted histograms and density plots of the variables using the same approach as for the boxplots.  First the plots, with default binwidths-->

<!--# Below, I used the `Freedman-Diaconis rule` to get the best 'binwdidth' for each variable.   Not easy to read, unless maximised.-->

```{r F-D_rule}
#
binwidth <- 2 * IQR(num_train$X1) / (length(num_train$X1)^(1/3))
binwidth
```

```{r FD_hist_train}
# calculate ideal binwidth for each variable in num_train, using Freedman-Diaconis Rule
binwidths <- apply(num_train, 2, function(x) {
  bw <- 2 * IQR(x) / (length(x)^(1/3))
  round(bw, 2)
})

# list of histograms for each variable
histograms <- lapply(names(num_train), function(x) {
  ggplot(data = num_train, aes(x = .data[[x]], y = after_stat(density))) +
    geom_histogram(binwidth = binwidths[x], fill = "#A3C4BC", color = "#616161") +
    geom_density(alpha = 0.5, color = "#0F4C5C") +
    labs(x = x, y = "") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
})

# histograms with cowplot package
plot_grid(plotlist = histograms, ncol = 3, align = "hv", axis = "tb") +
  ggtitle("Distribution of Variables in `num_train`") +
  theme(plot.title = element_text(hjust = 0.5, size = 16),
        plot.margin = unit(c(1, 1, 1, 1), "cm"))

```

<!--# In addition to visually inspecting the variables in terms of distributions and densities, I looked at a more formal approach to identify outliers in the dataset.  I calculated z-scores for each variable using 'scale()'.  -->

<!--# Then outliers where pulled out - those with a z-score of +/- 3 which is a common threshold.  In a normal distribution - 99.7% of observations fall within 3 standard deviations. -->

```{r}
# calculate z-scores for each variable
z_scores <- apply(num_train, 2, function(x) scale(x, center = TRUE, scale = TRUE))

# identify values with z-score greater than 3 or less than -3
outliers <- which(abs(z_scores) > 3, arr.ind = TRUE)
dim(outliers)

table_outliers <- table(outliers[, "col"])


# Print the summary table
print(table_outliers)


```

<!--# As was confirmed by visual inspection of IQR, most of the variables appear to be normally distributed although there are a few which are not normal.  The z-score approach suggests that there are 81 'outliers' in the dataset. -->

```{r}
par(mar = c(1, 1, 1, 1))
pairs.panels(train)
```

```{r}

# create correlation matrix
corr_matrix <- cor(num_train)

# plot correlation matrix using corrplot
corrplot(corr_matrix)

# plot scatterplot matrix using GGally
#ggpairs(train, progress=FALSE)

```

```{r}
ggplot(train, aes(x = X1, fill = label)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ label, nrow = 3)
```

```{r}
# Create a list of plots, with one plot for each variable
plot_list <- lapply(names(train), function(var) {
  ggplot(train, aes_string(x = var, fill = "label")) +
    geom_density(alpha = 0.5) +
    ggtitle(paste0("Distribution of ", var))
})

# Combine the plots into a grid using the cowplot package
plot_grid(plotlist = plot_list, ncol = 4)

```

```{r}
# order - order of ariables - hierarchical clustering
# method - correlation strength

# Compute the correlation matrix
corr_matrix <- cor(train[, 1:20])

# Plot the correlation matrix
corrplot(corr_matrix, method = "color", type = "upper", order = "hclust",
         col = colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))(200),
         addCoef.col = "black", tl.col = "black")
```

Observations \* many normal distributions \* some which differ across labels, eg. X7-X11

```{r}


train %>%
  select(label, X1, X2, X3, X4, X5) %>% 
  GGally::ggpairs(aes(color = label),
          columns = c("X1", "X2", "X3", "X4", "X5"), progress = FALSE) 

```

```{r}

train %>%
  select(label, X7, X8, X9, X10, X11) %>% 
  GGally::ggpairs(aes(color = label),
          columns = c("X7", "X8", "X9", "X10", "X11"), progress = FALSE) 

```

```{r}
corr_matrix <- cor(train %>% select(-label))

corrplot(corr_matrix, method = "circle")
```

## NOTES BELOW

so there appears to be value in two different approaches \* correlation for dimension reduction \* needs domain knowledge to clarify and ratifiy, explain

-   feature reduction - several variables which may be superfluous, measuring similar things or do not add to the model

To do \* normalise and scale \* check for outliers \* address outliers as appropriate \* correlation plot

To try:

```{r}
# correlation plot

corrplot(cor(num_train))
corrplot(cor(num_train), method = "ellipse", type = "lower")
```

interpretation - say what you see \* sparse - lots of variables which are not correlated with each other \* x17-20, and x7-10 appear to correlate, some neg, some pos

### 
