@article{blankersMissingDataApproaches2010,
  title = {Missing {{Data Approaches}} in {{eHealth Research}}: {{Simulation Study}} and a {{Tutorial}} for {{Nonmathematically Inclined Researchers}}},
  shorttitle = {Missing {{Data Approaches}} in {{eHealth Research}}},
  author = {Blankers, Matthijs and Koeter, Maarten W J and Schippers, Gerard M},
  year = {2010},
  month = dec,
  journal = {Journal of Medical Internet Research},
  volume = {12},
  number = {5},
  pages = {e54},
  issn = {1438-8871},
  doi = {10.2196/jmir.1448},
  urldate = {2023-03-22},
  abstract = {Background: Missing data is a common nuisance in eHealth research: it is hard to prevent and may invalidate research findings. Objective: In this paper several statistical approaches to data ``missingness'' are discussed and tested in a simulation study. Basic approaches (complete case analysis, mean imputation, and last observation carried forward) and advanced methods (expectation maximization, regression imputation, and multiple imputation) are included in this analysis, and strengths and weaknesses are discussed. Methods: The dataset used for the simulation was obtained from a prospective cohort study following participants in an online self-help program for problem drinkers. It contained 124 nonnormally distributed endpoints, that is, daily alcohol consumption counts of the study respondents. Missingness at random (MAR) was induced in a selected variable for 50\% of the cases. Validity, reliability, and coverage of the estimates obtained using the different imputation methods were calculated by performing a bootstrapping simulation study. Results: In the performed simulation study, the use of multiple imputation techniques led to accurate results. Differences were found between the 4 tested multiple imputation programs: NORM, MICE, Amelia II, and SPSS MI. Among the tested approaches, Amelia II outperformed the others, led to the smallest deviation from the reference value (Cohen's d = 0.06), and had the largest coverage percentage of the reference confidence interval (96\%). Conclusions: The use of multiple imputation improves the validity of the results when analyzing datasets with missing observations. Some of the often-used approaches (LOCF, complete cases analysis) did not perform well, and, hence, we recommend not using these. Accumulating support for the analysis of multiple imputed datasets is seen in more recent versions of some of the widely used statistical software programs making the use of multiple imputation more readily available to less mathematically inclined researchers.},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\WLQ725YN\\Blankers et al. - 2010 - Missing Data Approaches in eHealth Research Simul.pdf}
}

@article{brownChoosingRightType,
  title = {Choosing the {{Right Type}} of {{Rotation}} in {{PCA}} and {{EFA}}},
  author = {Brown, James Dean},
  langid = {english},
  keywords = {EFA,Factor Analysis,PCA},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\LMEGTBYD\\Brown - Choosing the Right Type of Rotation in PCA and EFA.pdf}
}

@book{changGraphicsCookbook2nd,
  title = {R {{Graphics Cookbook}}, 2nd Edition},
  author = {Chang, Winston},
  urldate = {2023-02-21},
  abstract = {This cookbook contains more than 150 recipes to help scientists, engineers, programmers, and data analysts generate high-quality graphs quickly\textemdash without having to comb through all the details of R's graphing systems. Each recipe tackles a specific problem with a solution you can apply to your own project and includes a discussion of how and why the recipe works.},
  keywords = {Graphics,R,Visualisations},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\WE5FMINX\\r-graphics.org.html}
}

@book{Chapter4MultipleImputation,
  title = {Chapter4 {{Multiple Imputation}} | {{Book}}\_{{MI}}.Knit},
  urldate = {2023-04-18},
  keywords = {Multiple Imputation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\4PUKB3BI\\multiple-imputation.html}
}

@misc{CiteSeerX,
  title = {{{CiteSeerX}}},
  journal = {CiteSeerX},
  urldate = {2023-04-30},
  howpublished = {https://citeseerx.ist.psu.edu/doc/10.1.1.650.2473},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\WKN5JI45\\10.1.1.650.html}
}

@misc{DimensionalityReductionPrincipal,
  title = {Dimensionality {{Reduction}}; {{Principal Component Analysis}} ({{PCA}})},
  urldate = {2023-02-17},
  abstract = {Data that includes many features or many different vectors can be thought of as having many dimensions. Often, it's useful to reduce those dimensions down to something more easily visualized, for compression, or to just distill the most important information from a data set (that is, information that contributes the most to the data's variance). Principal Component Analysis and Singular Value Decomposition do that.},
  langid = {english},
  keywords = {Dimension Reduction,PCA},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\UA5HM4X4\\9781787127081-video6_3.html}
}

@misc{doiSDS375,
  title = {{{SDS}} 375},
  author = {published yet DOI, Authors Affiliations Published Not},
  journal = {SDS 375},
  urldate = {2023-02-23},
  abstract = {Data Visualization in R},
  howpublished = {https://wilkelab.org/SDS375/},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\PJ466LYX\\SDS375.html}
}

@book{ForecastingPrinciplesPractice,
  title = {Forecasting: {{Principles}} and {{Practice}} (3rd Ed)},
  shorttitle = {Forecasting},
  urldate = {2023-03-28},
  abstract = {3rd edition},
  keywords = {Forecasting,Prediction,TimeSeries},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\3ZKA3ZA5\\fpp3.html}
}

@article{fraleyModelBasedClusteringDiscriminant2002,
  title = {Model-{{Based Clustering}}, {{Discriminant Analysis}}, and {{Density Estimation}}},
  author = {Fraley, Chris and Raftery, Adrian E},
  year = {2002},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {97},
  number = {458},
  pages = {611--631},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214502760047131},
  urldate = {2023-03-08},
  abstract = {Cluster analysis is the automated search for groups of related observations in a dataset. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures, and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as how many clusters are there, which clustering method should be used, and how should outliers be handled. We review a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, minefield detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology and discuss recent developments in model-based clustering for non-Gaussian data, high-dimensional datasets, large datasets, and Bayesian estimation.},
  keywords = {Bayes factor,Breast cancer diagnosis,Cluster analysis,Density Estimation,Discriminant Analysis,EM algorithm,Gene expression microarray data,Markov chain Monte Carlo,Mixture model,Model-Based Clustering,Outliers,Spatial point process},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\G3VN8PRC\\Fraley and Raftery - 2002 - Model-Based Clustering, Discriminant Analysis, and.pdf}
}

@article{FreedmanDiaconisRule2022,
  title = {Freedman\textendash{{Diaconis}} Rule},
  year = {2022},
  month = jul,
  journal = {Wikipedia},
  urldate = {2023-04-30},
  abstract = {In statistics, the Freedman\textendash Diaconis rule can be used to select the width of the bins to be used in a histogram. It is named after David A. Freedman and Persi Diaconis.  For a set of empirical measurements sampled from some probability distribution, the Freedman-Diaconis rule is designed roughly to minimize the integral of the squared difference between the histogram (i.e., relative frequency density) and the density of the theoretical probability distribution. The general equation for the rule is:                                   Bin width                  =         2                                                                          IQR                              (               x               )                                                          n                                    3                                                                                  \{\textbackslash displaystyle \{\textbackslash text\{Bin width\}\}=2\textbackslash,\{\{\textbackslash text\{IQR\}\}(x) \textbackslash over \{\textbackslash sqrt[\{3\}]\{n\}\}\}\}   where                         IQR         ⁡         (         x         )                 \{\textbackslash displaystyle \textbackslash operatorname \{IQR\} (x)\}    is the interquartile range of the data and                         n                 \{\textbackslash displaystyle n\}    is the number of observations in the sample                         x         .                 \{\textbackslash displaystyle x.\}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1101307943},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\6UWT9NXM\\Freedman–Diaconis_rule.html}
}

@book{gohelFlextableFunctionsTabular2023,
  title = {Flextable: {{Functions}} for {{Tabular Reporting}}},
  author = {Gohel, David and Skintzos, Panagiotis},
  year = {2023}
}

@book{grolemundDataScience2017,
  title = {R for {{Data Science}}},
  author = {Grolemund, Garrett and Wickham, Hadley},
  year = {2017},
  publisher = {{O'Reilly}}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  edition = {2nd edition}
}

@article{haytonFactorRetentionDecisions2004,
  title = {Factor {{Retention Decisions}} in {{Exploratory Factor Analysis}}: A {{Tutorial}} on {{Parallel Analysis}}},
  shorttitle = {Factor {{Retention Decisions}} in {{Exploratory Factor Analysis}}},
  author = {Hayton, James C. and Allen, David G. and Scarpello, Vida},
  year = {2004},
  month = apr,
  journal = {Organizational Research Methods},
  volume = {7},
  number = {2},
  pages = {191--205},
  publisher = {{SAGE Publications Inc}},
  issn = {1094-4281},
  doi = {10.1177/1094428104263675},
  urldate = {2023-02-11},
  abstract = {The decision of how many factors to retain is a critical component of exploratory factor analysis. Evidence is presented that parallel analysis is one of the most accurate factor retention methods while also being one of the most underutilized in management and organizational research. Therefore, a step-by-step guide to performing parallel analysis is described, and an example is provided using data from the Minnesota Satisfaction Questionnaire. Recommendations for making factor retention decisions are discussed.},
  keywords = {EFA,Parallel Analysis},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\IVZU3W4X\\Hayton et al. - 2004 - Factor Retention Decisions in Exploratory Factor A.pdf}
}

@article{osborneWhatRotatingExploratory,
  title = {What Is {{Rotating}} in {{Exploratory Factor Analysis}}?},
  author = {Osborne, Jason W.},
  publisher = {{University of Massachusetts Amherst}},
  doi = {10.7275/HB2G-M060},
  urldate = {2023-02-11},
  abstract = {Exploratory factor analysis (EFA) is one of the most commonly-reported quantitative methodology in the social sciences, yet much of the detail regarding what happens during an EFA remains unclear. The goal of this brief technical note is to explore what "rotation" is, what exactly is rotating, and why we use rotation when performing EFAs. Some commentary about the relative utility and desirability of different rotation methods concludes the narrative.},
  langid = {english},
  keywords = {EFA,Rotation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\EVG7DXPE\\Osborne - What is Rotating in Exploratory Factor Analysis.pdf}
}

@book{pedersenGgplot2ElegantGraphics,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Pedersen, Danielle Navarro, {and} Thomas Lin, Hadley Wickham},
  urldate = {2023-02-11},
  abstract = {A book created with bookdown.},
  langid = {english},
  keywords = {ggplot,R,Visualisation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\3T6R9A9I\\ggplot2-book.org.html}
}

@book{revellePsychProceduresPsychological2022a,
  title = {Psych: {{Procedures}} for {{Psychological}}, {{Psychometric}}, and {{Personality Research}}},
  author = {Revelle, William},
  year = {2022}
}

@book{riedererMarkdownCookbook,
  title = {R {{Markdown Cookbook}}},
  author = {Riederer, Christophe Dervieux, Emily, Yihui Xie},
  urldate = {2023-02-11},
  abstract = {This book showcases short, practical examples of lesser-known tips and tricks to helps users get the most out of these tools. After reading this book, you will understand how R Markdown documents are transformed from plain text and how you may customize nearly every step of this processing. For example, you will learn how to dynamically create content from R code, reference code in other documents or chunks, control the formatting with customer templates, fine-tune how your code is processed, and incorporate multiple languages into your analysis.},
  keywords = {Markdown,R,RMD},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\GR98LP8F\\rmarkdown-cookbook.html}
}

@misc{shlensTutorialPrincipalComponent2014,
  title = {A {{Tutorial}} on {{Principal Component Analysis}}},
  author = {Shlens, Jonathon},
  year = {2014},
  month = apr,
  number = {arXiv:1404.1100},
  eprint = {1404.1100},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-02-11},
  abstract = {Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,PCA,Statistics - Machine Learning},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\C2N27AXA\\Shlens - 2014 - A Tutorial on Principal Component Analysis.pdf;C\:\\Users\\zoona\\Zotero\\storage\\FBKCNXYQ\\1404.html}
}

@article{sterneMultipleImputationMissing2009,
  title = {Multiple Imputation for Missing Data in Epidemiological and Clinical Research: Potential and Pitfalls},
  shorttitle = {Multiple Imputation for Missing Data in Epidemiological and Clinical Research},
  author = {Sterne, Jonathan A. C. and White, Ian R. and Carlin, John B. and Spratt, Michael and Royston, Patrick and Kenward, Michael G. and Wood, Angela M. and Carpenter, James R.},
  year = {2009},
  month = jun,
  journal = {BMJ},
  volume = {338},
  pages = {b2393},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.b2393},
  urldate = {2023-04-18},
  abstract = {{$<$}p{$>$}Most studies have some missing data. \textbf{Jonathan Sterne and colleagues} describe the appropriate use and reporting of the multiple imputation approach to dealing with them {$<$}/p{$>$}},
  chapter = {Research Methods \&amp; Reporting},
  copyright = {\textcopyright{}  . This is an open-access article distributed under the terms of the Creative Commons Attribution Non-commercial License, which permits use, distribution, and reproduction in any medium, provided the original work is properly cited, the use is non commercial and is otherwise in compliance with the license. See: http://creativecommons.org/licenses/by-nc/2.0/  and  http://creativecommons.org/licenses/by-nc/2.0/legalcode.},
  langid = {english},
  pmid = {19564179},
  keywords = {epidemiological,imputation,Missing data},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\UC3AMSUS\\Sterne et al. - 2009 - Multiple imputation for missing data in epidemiolo.pdf;C\:\\Users\\zoona\\Zotero\\storage\\M8N9LCWE\\bmj.html}
}

@book{weiCorrplotVisualizationCorrelation2021a,
  title = {Corrplot: {{Visualization}} of a {{Correlation Matrix}}},
  author = {Wei, Taiyun and Simko, Viliam},
  year = {2021}
}

@book{weiPackageCorrplotVisualization2021a,
  title = {R Package 'Corrplot': {{Visualization}} of a {{Correlation Matrix}}},
  author = {Wei, Taiyun and Simko, Viliam},
  year = {2021}
}

@book{wickhamGgplot2CreateElegant2022a,
  title = {Ggplot2: {{Create Elegant Data Visualisations Using}} the {{Grammar}} of {{Graphics}}},
  author = {Wickham, Hadley and Chang, Winston and Henry, Lionel and Pedersen, Thomas Lin and Takahashi, Kohske and Wilke, Claus and Woo, Kara and Yutani, Hiroaki and Dunnington, Dewey},
  year = {2022}
}

@book{wickhamGgplot2ElegantGraphics,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Wickham, Hadley and Navarro, Danielle and Pedersen, Thomas Lin},
  edition = {work-in-progress 3rd edition}
}

@book{wickhamGgplot2ElegantGraphics2016a,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {{Springer-Verlag New York}},
  isbn = {978-3-319-24277-4}
}

@book{wilkeFundamentalsDataVisualization,
  title = {Fundamentals of {{Data Visualization}}},
  author = {Wilke, Claus O.},
  urldate = {2023-02-11},
  abstract = {A guide to making visualizations that accurately reflect the data, tell a story, and look professional.},
  keywords = {Data Visualisation,R},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\TVARL3EF\\dataviz.html}
}
