---
title: "Chemical Sample Classification Report"
#subtitle: "Modelling"
author: "Petter LÃ¶vehagen"
date: "09 May 2023"

RVersion: 3.6.0 
RStudio: 2023.03.0+386 Cherry Blossom Release
Platform: "x86_64-w64-mingw32/x64 (64-bit)"
OS: "Windows 11 SE"

format: 
  docx: default
  html:
    toc: true
    toc-depth: 3
    toc-title: "Contents"
    toc-location: left
    code-fold: true
    code-line-numbers: true
    theme: spacelab
    html-math-method: mathjax
    highlight: "tango"
    #css: "../data/custom.css"
output:
  word_document:
    fig.retina: 300
#prefer-docx: true
prefer-html: true
lang: "en-GB"
bibliography: "../data/AS_CW_References.bib"
csl: "../data/harvard-university-of-the-west-of-england.csl"

knitr:
  opts_chunk:
    include: false
    tidy: true
    echo: false
    warning: false
    message: false
    error: true
    #out.width: '90%'
    #fig.width: 6
    #fig.height: 4
---

```{r CL_model_packages}
# load/install R packages/libraries
# set seed
# load flextable defaults
# see R file for details
set.seed(567)
file <- "classify"
source("R/chem_report_load.R")


```

```{r cla_load_train_data}
# load in train dataset from csv files

# scaled train with labels
train_scaled_label <- read.csv("../data/train_scaled_label.csv")
train_label_vec <- train_scaled_label$label

# scaled train without labels 
train_scaled <- train_scaled_label[, -21]

# scaled train pca df
train_scaled_pca_df <- read.csv("../data/train_scaled_pca_df.csv")
```

```{r cla_load_validate_data}
# load in validate datasets from csv files

# read in valid dataset
valid_label <- read.csv("../data/chem_valid.csv")
valid_label_vec <- valid_label$label

# scaled validate without labels 
valid_scaled <- read.csv("../data/valid_scaled.csv")

# scaled validate with labels
valid_scaled_label <- cbind(valid_scaled, valid_label[21])

# scaled validate pca df
valid_scaled_pca_df <- read.csv("../data/valid_scaled_pca_df.csv")
```

<!--# In this section, we proceed with some classification models using both PCA and non-PCA datasets.  We start with KNN (K nearest neighbour) classifier.-->

```{r knn_base_model}
# create baseline KNN model with low K


# fit knn with k=3
sc_knn_3<-knn(train=train_scaled, test=valid_scaled, cl=train_label_vec, k = 3, prob=TRUE)

# confusion matrix
sc_knn_3_tbl <- table(truth = valid_label_vec, predicted=sc_knn_3[1:length(valid_label_vec)])

#sc_knn_3_tbl

#  kable table
kbl(sc_knn_3_tbl)|>kable_classic(full_width = F, html_font = "Cambria")
```

<!--# The baseline knn is not great, especially when it comes to predicting for label E.  The confusion matrix shows that the model has both false positives and true negatives - that is, it is predicting E when it is not E and predicting not-E when in fact, the true label is E. -->

<!--# Let us consider more values for K and print confusion matrices for each, as well as storing the accuracies.  It common to check up to the value of the square root of the observations, for the value of K.-->

## Classification Models {#sec-classification-models}

### PCA Performance {#sec-pca-performance}

Three classifiers were developed and fitted with PCA and non-PCA data to compare results: `k nearest neighbour` (KNN), `model based discriminant analysis` (DA) and `support vector machines` (SVM). 

Models were trained and then validated on 'unseen' data. Accuracy (correct model predictions) was the performance metric.


#### K Nearest Neighbour {#sec-k-nearest-neighbour}



```{r knn_loop, include=FALSE, echo=FALSE }

# loop through k, print confusion matrix and store accuracies

# square root N of dataset to get upper K
K = sqrt(length(train_scaled[,1]))

k_values <- 4:K
# empty vectors to store results
accuracies <- rep(0, length(k_values))

# loop over different values of K, storing accuracies
for (i in seq_along(k_values)) {
  k <- k_values[i]
  # pull each each knn
  pull <- knn(train=train_scaled, test=valid_scaled, cl=train_label_vec, k=k, prob=TRUE)[1:length(valid_label_vec)]
  
  # create kable table
  tab <- kbl(table(truth=valid_label_vec,predicted=pull)) |>
    kable_classic(full_width = F, html_font = "Cambria")
  # print each
  print(tab)
  
  # store each accuracy = predict == actual/total
  acc <- sum(pull == valid_label_vec) / length(valid_label_vec)
  accuracies[i] <- acc
}

# create a df of results
knn_accuracies <- data.frame(K=k_values, Accuracy=accuracies)


#knn_accuracies
```

<!--# A quick scroll through the confusion matrices for K up to 35 and it is evident that the size of K does not improve the performance of the model.  More K does not mean better classification, especially of label E.  The other labels are being classified relatively well but there is no convergence on group E. -->

```{r best_knn_accuracy}

# max accuracy
max_accuracy <- max(knn_accuracies$Accuracy)

# associated k 
best_k <- knn_accuracies$K[which.max(knn_accuracies$Accuracy)]

# print
cat("Best accuracy:\n")
round(max_accuracy,2)
cat("K for best accuracy:\n")
best_k
```
KNN classifies samples into groups by 'distance' to neighbours. Overall, non-PCA models performed better with a best default accuracy of `r round(max_accuracy,2)`% compared to 73.5%.

<!--# The best accuracy achieved by the knn classifier is 82.4% which was with k = 7. This is visualised below. -->

```{r plot_knn_accuracy}

# create a plot
knn_acc <- ggplot(knn_accuracies, aes(x=K, y=Accuracy)) +
  geom_line() +
  geom_point() +
  labs(x="K", y="Accuracy", title="", subtitle = "Scaled") +
  geom_vline(xintercept = 7, colour = "red", linetype = "dashed") +
  annotate("text", x = 7.5, y = max_accuracy, label = paste0("Accuracy = ", round(max_accuracy*100, 1), "%"), color = "darkgreen", hjust = 0, vjust = 0) +
  ylim(0.65, 0.85)

```

<!--# Below is the same loop but for the PCA transformed dataset. -->

```{r knn_pca_loop}

# loop through k, print confusion matrix and store accuracies for PCA transformed data

# square root N of dataset to get upper K
K = sqrt(length(train_scaled[,1]))

k_values <- 4:K
# empty vectors to store results
accuracies <- rep(0, length(k_values))

# loop over different values of K, storing accuracies
for (i in seq_along(k_values)) {
  k <- k_values[i]
  # pull each each knn
  pull <- knn(train=train_scaled_pca_df, test=valid_scaled_pca_df, cl=train_label_vec, k=k, prob=TRUE)[1:length(valid_label_vec)]
  
  # create kable table
  tab <- kbl(table(truth=valid_label_vec,predicted=pull)) |>
    kable_classic(full_width = F, html_font = "Cambria")
  # print each
  print(tab)
  
  # store each accuracy = predict == actual/total
  acc <- sum(pull == valid_label_vec) / length(valid_label_vec)
  accuracies[i] <- acc
}

# create a df of results
knn_pca_accuracies <- data.frame(K=k_values, Accuracy=accuracies)
```

```{r best_knn_pca}
# max pca accuracy
max_pca_accuracy <- max(knn_pca_accuracies$Accuracy)
# associated k value
best_pca_k <- knn_pca_accuracies$K[which.max(knn_pca_accuracies$Accuracy)]

# print
cat("Best accuracy:\n")
round(max_pca_accuracy, 2)
cat("K for best accuracy:\n")
best_pca_k
```
KNN classifies samples into groups by 'distance' to neighbours. Overall, non-PCA models performed better with a best default accuracy of `r round(max_accuracy,2)`% compared to `r round(max_pca_accuracy, 2)`%.

```{r plot_knn_pca_accuracy}


# create a plot
knn_pca_acc <- ggplot(knn_pca_accuracies, aes(x=K, y=Accuracy)) +
  geom_line() +
  geom_point() +
  labs(x="K", y="Accuracy", title="KNN Classifier Accuracy", subtitle = "Scaled and PCA Transformed") + 
  geom_vline(xintercept= 7, linetype = "dashed", colour = "red") + 
  geom_vline(xintercept = 7, colour = "red", linetype = "dotted") +
  annotate("text", x = 7.5, y = max_pca_accuracy, label = paste0("Accuracy = ", round(max_pca_accuracy*100, 1), "%"), color = "darkgreen", hjust = 0, vjust = 0) +
  ylim(0.65, 0.85)




```

```{r compare_knn_plot, include=TRUE, warning= FALSE, echo=FALSE, message = FALSE, tab.align='left', fig.alt="Plot KNN classifier accuracies for different K in PCA and non-PCA training data"}
#| label: fig-compare-knn-plot
#| fig-cap: "KNN classifier accuracies for different K"

(knn_pca_acc + knn_acc)
```

#### Discriminant Analysis {#sec-discriminant-analysis}



<!--# KNN models for both PCA and non-PCA data have their best accuracy at K = 7 but the accuracy is almost 10% better with the un-transformed dataset. -->

<!--# Below I fitted a linear discriminant analysis model using mclust and plotted some results.  The plots have been commented out in the interest of speed.  To run these, remove the #.  -->

```{r model_DA}

# fit discriminant analysis model on non-pca data
MBDA_sc <- MclustDA(data=train_scaled, class=train_label_vec,verbose=FALSE)

# save summary
MBDA_sc_sum <-summary(MBDA_sc, parameters = TRUE)

# print summary
MBDA_sc_sum
```

<!--# The default mclust model on the scaled dataset show that no valid model was identified for labels A, B, D, E - instead the algorithm defaulted to basic univariate modelling which essentially means that each variable is treated separately and labels (groups) are assigned on the marginal probability of each variable, assuming that the variables are independent and have equal variance. Label E, however had a specific model - EEI - which means it is a model of equal covariance matrix with unconstrained dimensions and there are three clusters.-->

<!--# The model performed well when looking at the confusion matrix.  It had a classification error of 0.0278 and a Brier score of 0.024.  The classification error is the sum of off-diagonal elements divided by total observations - meaning that it has high predictive accuracy.  Brier score is another measure of accuracy based on probabilistic predictions but taking into account correctness and confidence of predictions.  Brier score penalises for over-confidence and rewards for correct predictions.  A low Brier score indicates a better performing model - so 0.024 is good.  -->

<!--# Again it is label E which s challenging.  The model still predicts 218/236 which is still good at 92.4% correct. Other statistics include the log-likelihood where a higher value means a better fitting model; BIC (Bayesian information criterion) where a lower value is better.  The low BIC value suggests that this model fits the data well and is not overly complex. -->

```{r plot_DA}
# these plots are not particularly useful - similar to pair plots from earlier in the report.

# # pair plot
# plot(MBDA_sc, what = "classification")
# # plot x1, x2
# plot(MBDA_sc, what = "classification", dimens = c(1,2))
#  # plot x18, x19 - linear
# plot(MBDA_sc, what = "classification", dimens = c(18,19))
```

```{r density_plot}

# these density plots have been commented out as they are not needed at this time for this model.

# dens_train_sc <- densityMclust(train_scaled, verbose = FALSE)
# 
# plot(dens_train_sc, what ="BIC")
```

<!--# It is useful to confirm this results on our 'validate' set - this is unseen data, so we can confirm how well the model generalises to new data.  The results are very good - with an accuracy of 96.4% - only slightly worse then the accuracy on the train dataset (97.2%).  The Brier score on the validate set is 0.0311, also very good. -->

```{r MBDA_sc_accuracy}
# make predictions on the validation set
valid_preds <- predict(MBDA_sc, newdata = valid_scaled)
valid_preds <- valid_preds$classification
  # calculate accuracy of predictions
da_sc_accuracy <- sum(valid_preds == valid_label_vec)/ length(valid_label_vec)

cat("Best accuracy (DA):\n")  
da_sc_accuracy

round((da_sc_accuracy *100),2)
```


```{r model_DA_valid}
# fit and store DA validate dataset 
sum_MBDA_sc<-summary(MBDA_sc, newdata = valid_scaled, newclass = valid_label_vec)

# print results
sum_MBDA_sc
```

```{r model_DA_output}


#MBDA_sc$models$
# model name for A group
#MBDA_sc$models[[1]]$modelName

# model names for each group
MBDA_sc_names<-unlist(lapply(MBDA_sc$models,function(x){x$modelName}))

# Gs for each group
MBDA_sc_G<-unlist(lapply(MBDA_sc$models,function(x){x$G}))

# glue into output
glue_collapse(names(MBDA_sc_names),sep=", ",last=" and ")
glue_collapse(MBDA_sc_names,sep=", ",last=" and ")
glue_collapse(MBDA_sc_G,sep=", ",last=" and ")
```

<!--# The same model has been fitted to the PCA transformed data for a comparison.  The results on the PCA training set are signifcantly worse than un-transformed training set with a classification error of 0.25 and a Brier score of 0.17. This means that the accuracy is only 75%.  The usual suspect - label E - is very poorly classified but there is also misclassification for label B, C and D.  It seems likely that four components may not have captured enough of the variability in the dataset. -->

```{r model_DA_pca}

# fit model DA on pca transformed data
MBDA_pca <- MclustDA(data=train_scaled_pca_df, class=train_label_vec,verbose=FALSE)

# print output
summary(MBDA_pca, parameters = TRUE)

```

```{r MBDA_pca_accuracy}
# make predictions on the validation set
valid_pca_preds <- predict(MBDA_pca, newdata = valid_scaled_pca_df)
valid_pca_preds <- valid_pca_preds$classification
  # calculate accuracy of predictions
DA_pca_accuracy <- sum(valid_pca_preds == valid_label_vec)/ length(valid_label_vec)

cat("Best accuracy (PCA):\n")  
DA_pca_accuracy
```
Discriminant Analysis classification creates groups from mixed mathematical models based on different variable means.  The default model had accuracies of `r round((da_sc_accuracy *100),2)`% (non-PCA) and `r round((DA_pca_accuracy*100), 2)`% on the 'validation' dataset. 

Label E is particularly challenging to classify with false positives and false negative misclassifications.


```{r plot_DA_pca}
# DA pca plots
plot(MBDA_pca, what = "classification")
plot(MBDA_pca, what = "classification", dimens = c(1,2))
```

#### Support Vector Machines {#sec-support-vector-machines}

<!--# Another model to investigate is the support vector machine (SVM) - which provides a 90.7% classification accuracy initially.This was fitted to the PCA dataset as well, which produced an accurace of 69.8% which confirms that PCA is probably not suitable for the final classifier.-->

```{r svm_model}

# svm model with linear kernel
svm_model <- svm(as.factor(train_label_vec) ~ ., data = train_scaled, kernel = "linear")

# predictions on the valid set
valid_preds <- predict(svm_model, valid_scaled)

# accuracy of the predictions
svm_accuracy <- sum(valid_preds == valid_label_vec) / length(valid_label_vec)

cat("Best accuracy (SVM):\n")
svm_accuracy
```

```{r svm_pca}

# SVM model with a linear kernel
svm_model_pca <- svm(as.factor(train_label_vec) ~ ., data = train_scaled_pca_df, kernel = "linear")

# predictions training set
valid_preds <- predict(svm_model_pca, valid_scaled_pca_df)

# accuracy of the predictions
svm_pca_accuracy <- sum(valid_preds == valid_label_vec) / length(valid_label_vec)

cat("Best accuracy - pca (SVM):\n")
svm_pca_accuracy
```

SVM which did not perform as well as DA or KNN. The non-PCA model had an accuracy of `r round((svm_accuracy*100),2)`% and the PCA model resulted in an accuracy of only `r round((svm_pca_accuracy*100),2)`%.


::: callout-important
## Decision

PCA is not appropriate on this dataset as it does not capture enough variability for a successful classifier.
:::

### Feature Reduction {#sec-feature-reduction}

An alternative to dimension reduction is `feature reduction` - removing variables. Some variables may not add explanatory power because they capture similar information to other variables or they do not contain relevant information.

<!--# At this point, trees and forests will be explored and considered for this report -->

```{r single_tree_train}
# create tree on train dataset
tree_train <- rpart(label~., data = train_scaled_label)

# print
print(tree_train, digits = 2)

# plot
plot(tree_train)
# add label
text(tree_train, xpd=TRUE)
```

```{r treemax}
# maximum tree
tree_train_max<-rpart(label~., data = train_scaled_label,
               minsplit=2,cp=0)

#plot 
plot(tree_train_max)
tree_train_max$cptable

# plot complexity
plotcp(tree_train_max)
#View(tree_train_max$cptable)
```

<!--# The complexity plot above allows us to find the point (elbow) where relative error is lowest in relation to the complexity - which is 0.0034. -->

```{r tree_complexity}

# optimal complexity
opt_complex<-tree_train_max$cptable[which.min(tree_train_max$cptable[,4]),1]
opt_complex

# finding split of optimal complexity
split_tree<-tree_train_max$cptable[which.min(tree_train_max$cptable[,4]),2]
split_tree

# finding best place to prune programmatically
opt_tree<-prune(tree_train_max,cp=opt_complex)
plot(opt_tree)
text(opt_tree,xpd=TRUE,cex=.5)
```

```{r tree_threshold}

# find threshhold of 1 standard error 
threshold_1SE<-sum(tree_train_max$cptable[which.min(tree_train_max$cptable[,4]),4:5])

# find corresponding complexity
complex_1SE<-tree_train_max$cptable[min(which(tree_train_max$cptable[,4]<=threshold_1SE)),1]

# prune tree at complexity of 1SE
tree_1SE<-prune(tree_train_max,cp=complex_1SE)
plot(tree_1SE)
text(tree_1SE,xpd=TRUE,cex=.75)
```

<!--# Looking at the above trees, it is clear that classifying E is the challenge.  Class A is isolated with two variables - X9 and X7 but E and to a much lesser extent, the other labels are spread widely. -->

```{r missclassification_trees}

# looking at missclassifications on the three trees

#  unpruned tree
err_tree_valid_max <- mean(predict(tree_train_max, valid_scaled, type = "class") != valid_scaled_label$label)
err_tree_train_max <- mean(predict(tree_train_max, train_scaled_label, type = "class") != train_scaled_label$label)

# optimal tree 
err_tree_valid_opt <- mean(predict(opt_tree, valid_scaled, type = "class") != valid_scaled_label$label)
err_tree_train_opt <- mean(predict(opt_tree, train_scaled_label, type = "class") != train_scaled_label$label)

# pruned tree with 1SD complexity
err_tree_valid_max_1SE <- mean(predict(tree_1SE, valid_scaled, type = "class") != valid_scaled_label$label)
err_tree_train_max1SE <- mean(predict(tree_1SE, train_scaled_label, type = "class") != train_scaled_label$label)

cat("Misscalculation error rate for validate (unpruned):\n")
err_tree_valid_max
cat("\nMisscalculation error rate for train (unpruned):\n")
err_tree_train_max

cat("\nMisscalculation error rate for validate (optimal):\n")
err_tree_valid_opt
cat("\nMisscalculation error rate for train (optimal):\n")
err_tree_train_opt

cat("\nMisscalculation error rate for validate (pruned - 1SD complexity):\n")
err_tree_valid_max_1SE
cat("\nMisscalculation error rate for train (pruned- 1SD complexity):\n")
err_tree_train_max1SE
```

<!--# By comparing error rates on th the three trees, we can see that the unpruned tree performs best and as expected is completely overfitted to the train set.  The optimal tree has an accuracy of 90.5% on the valid set while the 1 standard deviation tree has an accuracy of 89.7%. Thus, we see that pruning the tree hits accuracy but it has the benefit of curtailing overfitting whilst retaining predictive power.-->

<!--# Trees can help understand the importance of individual variables by looking at which ones are used to split the branches.  The barchart below shows the most important variables are X9, X7, X8, X10, followe by X11, X3, X1, X6 and then they sart to have less of an impact.  Removing features (variables) will be looked at - so this is useful information. -->

```{r variable_importance_bar}

# complexity parameter at the first split
cp1split <- tree_train_max$cptable[2, 1]

# prune using the cp value at the first split
tree1split <- prune(tree_train_max, cp = cp1split)

# visualise 
plot(tree1split)
text(tree1split, xpd = TRUE, cex = 0.75)

# calculate misclassification error rate on train and valid
errTestTree1split <- mean(predict(tree1split, valid_scaled, type = "class") != valid_scaled_label$type)
errTrainTree1split <- mean(predict(tree1split, train_scaled_label, type = "class") != train_scaled_label$type)

# visualise the variable importance plot

barplot(tree_train_max$variable.importance, las = 2, cex.names = 0.6)
```

<!--# One thing to try is bootstrapping data samples and trees so that the accuracy can be assessed.  The general principle is to calculate the error difference between trees - if it is high - then it could indicate that the model is unstable.  For example, the below two trees have a difference of 17% which is quite high...but it is only two random samples, so bootstrapping several hundred will get closer to the mean. -->

```{r boot_try}

set.seed(511)
## bootstrap samples.

# create train boot
train_boot1 <- train_scaled_label[sample(1:nrow(train_scaled_label),                          nrow(train_scaled_label),replace=TRUE),]

# create tree
tree_boot1 <- rpart(label~., data=train_boot1)


# create train boot 2
train_boot2 <- train_scaled_label[sample(1:nrow(train_scaled_label),                          nrow(train_scaled_label),replace=TRUE),]

# create tree 2
tree_boot2 <- rpart(label~., data=train_boot2)

mean(predict(tree_boot1,valid_scaled,type="class")!=
       predict(tree_boot2,valid_scaled,type="class"))
```

```{r bootstrap}
set.seed(567)

#vector to store the bootstrapped accuracies
boot_accuracy <- vector()

#iterate trees
for (i in 1:200) {

  #bootstrapped training dataset
  train_boot <- train_scaled_label[sample(1:nrow(train_scaled_label), nrow(train_scaled_label), replace = TRUE), ]

  #decision tree from the bootstrapped training dataset
  tree_boot <- rpart(label ~ ., data = train_boot)

  #predict labels of the validation dataset using the decision tree
  pred_boot <- predict(tree_boot, valid_scaled, type = "class")

  #accuracy of the decision tree on the validation dataset
  boot_accuracy[i] <- mean(pred_boot == valid_scaled_label$label)
}

#mean bootstrapped accuracy
print(mean(boot_accuracy))
```

```{r bootstrap2}
set.seed(567)
# iterate samples over multiple trees - e.g. 200 x 5 = 1000

# bootstrap samples and trees.
nBoot <- 200
nTrees <- 5

# classification error matrix.
err_matrix <- matrix(NA, nrow=nBoot, ncol=nTrees)

# iterate.
for (i in 1:nBoot) {
  # bootstrap sample.
  trainBoot <- train_scaled_label[sample(1:nrow(train_scaled_label), nrow(train_scaled_label),replace=TRUE),]
  
  # multiple decision trees on sample
  for (j in 1:nTrees) {
    treeBoot <- rpart(label ~ ., data=trainBoot)
    err_matrix[i,j] <- mean(predict(treeBoot, valid_scaled, type="class") != valid_scaled_label$label)
  }
}

#  mean classification error 
errMean <- apply(err_matrix, 1, mean)

# standard deviation of the classification error 
errSD <- apply(err_matrix, 1, sd)

# overall mean classification error and standard deviation 
errOverall <- mean(errMean)
errSDOverall <- sd(errMean)
```

```{r erroroveralls}
cat("Overall error:\n")
errOverall
cat("\nOverall SD Error:\n")
errSDOverall
```

<!--# The bootstrapping suggests that the trees are not that stable - with an overall error of 15% on average. -->

<!--# Trees are usually not the best classifiers, but 'random forests' can be. This first on is not great - with an error rate of 10.29%. -->

```{r first_forest}

# number of splits, max is number of variables
mtry = ncol(train_scaled_label)-1

#create tree
bagging <- randomForest(as.factor(label)~.,data = train_scaled_label,mtry=mtry)

# results
bagging
```

```{r misclass}
# misclassification on validation
err_valid_bagging<-mean(predict(bagging,valid_scaled)!=valid_scaled_label$label)

# misclassification on train
err_train_bagging<-mean(predict(bagging,train_scaled)!=train_scaled_label$label)

cat("Misclassification error on validation:\n")
err_valid_bagging
cat("Misclassification error on train:\n")
err_train_bagging
```

```{r default_forest}
# out of box, default tree

rf_default <-randomForest(as.factor(label)~.,data = train_scaled_label)
rf_default

# valid error
errValid_rf_default<-mean(predict(rf_default,valid_scaled)!=valid_scaled_label$label)

# train error
errTrain_rf_default<-mean(predict(rf_default,train_scaled)!=train_scaled_label$label)

cat("\nMisclassification Error (Valid)\n")
errValid_rf_default
cat("Misclassification Error (Train)\n")
errTrain_rf_default

# plot
plot(rf_default)
```

<!--# The default forest has a misclassification error rate of 3.6% on the validate set and an estimated out of bag error rate of 8.65% - how it will generalise to unseen data.  Again, it is label E wich is difficult to classify.  Is it really a label? -->

```{r forest_importance}

# same fores with importance = true
rf_imp <-randomForest(as.factor(label)~.,data = train_scaled_label, importance = TRUE)

# plot variable importance
varImpPlot(rf_imp,type=1,scale=FALSE,
           n.var=ncol(train_scaled),cex=.5,
           main="Variable Importance")

# update plot 
varImpPlot(rf_imp,type=1,scale=FALSE,
           n.var=20,cex=.75,
           main="Variable Importance")

par(mar=c(4,2,2,1)+.1)

# plot top 15
var_imp_12<-varImpPlot(rf_imp,type=1,scale=FALSE,
           n.var=12,cex=1,
           main="Top 12 Variable Importance")

# predict using rf_imp
RFprob<-predict(object=rf_imp, newdata = valid_scaled, type = "prob")
# calculate brier score for random forest
BrierScore(z=RFprob, class=valid_scaled_label$label)
```

<!--# Looking at the variable importance from random forests, it is evident that X9, X7, X8, X10 are the most important variables, followed by X11, X3 and others so this should be explored in terms of feature reduction.-->

Clustering techniques like trees and random forests, can help identify candidate variables for inclusion / exclusion. @fig-var-imp shows taht variables X7-X10 are more important when splitting into labels.


```{r var_imp_plot, include=TRUE, echo=FALSE, message = FALSE, tab.align='left', fig.alt="Plot with variable importance using random forest"}
#| label: fig-var-imp
#| fig-cap: "Variable importance using random forest"

# font size and type
#par(family = "sans", cex.lab = 1.2, cex.axis = 1.2, cex.main = 1.5)

# var imp plot 
varImpPlot(rf_imp, type = 1, scale = FALSE, n.var = 12, cex = 1, main = "Top 12 Variable Importance",
           col = "blue", pch = 20)




```

```{r create_reduced_datasets}

#train_scaled_label

train_1 <- train_scaled_label[, c("X7", "X8", "X9", "X10")]
train_2 <- cbind(train_1, train_scaled_label[, c("X3", "X1")])
train_3 <- cbind(train_1, train_scaled_label[, c("X3", "X11")])
train_4 <- cbind(train_2, train_scaled_label[, c("X2", "X13")])
train_5 <- cbind(train_3, train_scaled_label[["X16"]])
colnames(train_5)[ncol(train_5)] <- "X16"

colnames(train_scaled_label)
# valid_scaled


valid_1 <- valid_scaled_label[, c("X7", "X8", "X9", "X10")]
valid_2 <- cbind(valid_1, valid_scaled_label[, c("X3", "X1")])
valid_3 <- cbind(valid_1, valid_scaled_label[, c("X3", "X11")])
valid_4 <- cbind(valid_2, valid_scaled_label[, c("X2", "X13")])
valid_5 <- cbind(valid_3, valid_scaled_label[["X16"]])
colnames(valid_5)[ncol(valid_5)] <- "X16"
```
Reduced variable datasets to test performance:

| Set   | Included Variables |
|-------|--------------------|
| Set 1 | `r names(train_1)` |
| Set 2 | `r names(train_2)` |
| Set 3 | `r names(train_3)` |
| Set 4 | `r names(train_4)` |
| Set 5 | `r names(train_5)` |

: Reduced Feature Datasets

### Model Performance {#sec-model-performance}

Models were optimsed by searching through combinations of `hyperparameters` on datasets with different variables.

```{r mclust_dim_red}

# this has been disabled as it requires input from the user
# however looking at the 'classification' plots - it is easier to see the clusters.  blue is particularly obvious - most likely label A - and orange is most likely label E as it is very spread out and on top of all other labels.

# store mclust dimension reduction
pca_result <- MclustDR(MBDA_sc)

#plot dimension reduction 
#plot(pca_result)
```

<!--# On the basis of the results of knn and mclust discriminant analysis classification models, it appears that PCA transformed data is not as good at classifying these chemicals as the un-transformed dataset, so PCA is abandoned in the modelling.  Only scaled data will be considered from here. -->

<!--# Below is the first attempt at tuning the hyperparameters of the Mclust DA model.  It uses lower BIC as its improvement metric and I found that the BIC keeps lowering even after the accuracy worsens, as n.components increases.  This is likely because the model is overfitting - that is, it is improving on the 'train' dataset but at the expense of generalising to unseen data. In terms of the best model, n.components is 11 with EEI models for each label.  The Brier scores are 0.023 and 0.034 for train and validate, respectively.  The classification errors are 0.033 and 0.041 respectively. It has been commented out in the interest of faster report knitting.-->

```{r mclust_tune1}

# 
# 
# # number of mixture components
# 
# #n.components <- c(2, 3, 4, 5, 6, 7, 8, 9, 10)
# 
# #n.components <- c(10, 12, 14, 16, 18, 20)
# #n.components <- c(20, 22, 24, 26, 28, 30)
# n.components <- c(11)
# # grid to search
# hyperparameters <- expand.grid(n.components=n.components)
# 
# # initialise the best
# best.model <- NULL
# best.bic <- Inf
# 
# # loop over grid 
# for (i in 1:nrow(hyperparameters)) {
# 
#   # train_scaled to a numeric matrix
#   data <- as.matrix(train_scaled)
# 
#   # fit mclust model 
#   model <- MclustDA(data=data, class=train_label_vec, G=hyperparameters$n.components[i], verbose=FALSE)
#   
#   # bic
#   sum <- summary(model)
#   bic <- sum$bic
# 
#   # better bic, saved
#   if (bic < best.bic) {
#     best.model <- model
#     best.bic <- bic
#   }
# }
# 
# # best model
# sum_best_model <- summary(best.model, newdata = valid_scaled, newclass = valid_label_vec)
# 
# # best bic
# cat("Best bic:\n")
# print(sum_best_model$bic)
# cat("\nModel with best bic:\n")
# print(sum_best_model)
# 
# 
# # make predictions on the validation set
# valid_preds <- predict(best.model, newdata = valid_scaled)
# valid_preds <- valid_preds$classification
#   # calculate accuracy of predictions
# accuracy <- sum(valid_preds == valid_label_vec)/ length(valid_label_vec)
# 
# cat("\nBest accuracy (DA):\n")  
# accuracy

```

<!--# Version 2 of hyperparameter tuning can be found below. In addition to n.components, this searches over model names and diagonal as well.  It is also based on BIC scores.-->

```{r mclust_tune2}

# 
# # hyperparameters to search
# #n.components <- c(2, 3, 4, 5, 6)
# n.components <- c(6, 7, 8, 9, 10)
# diagonal <- c(TRUE, FALSE)
# modelName <- c("VEV", "EII", "VII", "EEE", "VVV", "EEI", "VEI", "EVI", "VVI", "VEE", "EVE", "VVE", "EEV", "VEV", "EVV"  )
# 
# 
# # grid to search
# hyperparameters <- expand.grid(n.components = n.components,
#                                diagonal = diagonal,
#                                modelName = modelName
#                                )
# 
# # initialise the best
# best.model <- NULL
# best.bic <- Inf
# 
# # loop over grid 
# for (i in 1:nrow(hyperparameters)) {
# 
#   # train_scaled to a numeric matrix
#   data <- as.matrix(train_scaled)
# 
#   # fit mclust model 
#   model <- MclustDA(data = data,
#                     class = train_label_vec,
#                     G = hyperparameters$n.components[i],
#                     diagonal = hyperparameters$diagonal[i],
#                     modelName = hyperparameters$modelName[i],
#                     verbose = FALSE)
#   
#   # bic
#   sum <- summary(model)
#   bic <- sum$bic
# 
#   # better bic, saved
#   if (bic < best.bic) {
#     best.model <- model
#     best.bic <- bic
#   }
# }
# 
# # best model
# sum_best_model2 <- summary(best.model, newdata = valid_scaled, newclass = valid_label_vec)
# 
# # results
# cat("Best BIC:\n")
# print(sum_best_model2$bic)
# cat("Model with best BIC:\n")
# print(sum_best_model2)
# 
# # make predictions on the validation set
# valid_preds2 <- predict(best.model, newdata = valid_scaled)
# valid_preds2 <- valid_preds2$classification
# 
# # calculate accuracy of predictions
# accuracy2 <- sum(valid_preds2 == valid_label_vec)/ length(valid_label_vec)
# 
# cat("Best accuracy (DA):\n")  
# accuracy2
```

<!--# The results of this loop have overtrained massively.  The model is almost perfect on the train dataset with only one misclassification (E as B) but it is poor on the validate set with a classification error of 11.8%. -->

<!--# The below is a Quadratic Discriminant Analysis model using Mclust.  QDA only supports EII models.  It is not as good as the previous models, with an accuracy of 85.1%.  It has been commented out in the interest of report knitting speed.-->

```{r mclust_tune3_qda}
# # hyperparameters to search
# n.components <- c(2, 3, 4, 5, 6)
# modelName <- "EII"  # QDA only supports "EII" model
# hyperparameters <- expand.grid(n.components = n.components, modelName = modelName)
# 
# # initialise the best
# best.model <- NULL
# best.bic <- Inf
# 
# # loop over grid 
# for (i in 1:nrow(hyperparameters)) {
# 
#   # train_scaled to a numeric matrix
#   data <- as.matrix(train_scaled)
# 
#   # fit mclust model 
#   model <- MclustDA(data = data,
#                     class = train_label_vec,
#                     G = hyperparameters$n.components[i],
#                     modelName = hyperparameters$modelName[i],
#                     verbose = FALSE)
#   
#   # bic
#   sum <- summary(model)
#   bic <- sum$bic
# 
#   # better bic, saved
#   if (bic < best.bic) {
#     best.model <- model
#     best.bic <- bic
#   }
# }
# 
# # best model
# sum_best_model <- summary(best.model, newdata = valid_scaled, newclass = valid_label_vec)
# 
# # results
# cat("Best BIC:\n")
# print(sum_best_model$bic)
# cat("Model with best BIC:\n")
# print(sum_best_model)
# 
# # make predictions on the validation set
# valid_preds <- predict(best.model, newdata = valid_scaled)
# valid_preds <- valid_preds$classification
# 
# # calculate accuracy of predictions
# accuracy <- sum(valid_preds == valid_label_vec) / length(valid_label_vec)
# 
# cat("\nBest accuracy (QDA):\n")  
# accuracy
```

<!--# As with the Mclust DA models, the SVM can be hypertuned - the best model has an acuracy of 95.6% -->

```{r tune_svm}

# # use gridsearch method to check across hyperparameters, to get best svm for scaled only data
# 
# # range of hyperparameters to check
# hyperparameters <- expand.grid(C = c(0.01, 0.1, 1, 10, 100),
#                                kernel = c("linear", "polynomial", "radial", "sigmoid"))
# 
# # initialise variables for best
# best_accuracy <- 0
# best_model <- NULL
# 
# # grid search
# for (i in 1:nrow(hyperparameters)) {
#   # train SVM model hyperparameters - cost, kernel
#   svm_model <- svm(as.factor(train_label_vec) ~ ., data = train_scaled, kernel = hyperparameters$kernel[i], 
# cost = hyperparameters$C[i])
#   
#   # predict on validation data
#   valid_preds <- predict(svm_model, valid_scaled)
#   
#   # accuracy of predictions
#   accuracy <- sum(valid_preds == valid_label_vec) / length(valid_label_vec)
#   
#   # update best model if higher accuracy
#   if (accuracy > best_accuracy) {
#     best_accuracy <- accuracy
#     best_model <- svm_model
#   }
# }
# 
# # print information about best model
# cat("Best model:\n")
# print(best_model)
# cat("Accuracy:", best_accuracy, "\n")
```

```{r knn_reduced_set1}

# # loop through k, print confusion matrix and store accuracies
# 
# 
# # square root N of dataset to get upper K
# K = sqrt(length(train_1[,1]))
# 
# k_values <- 1:K
# # empty vectors to store results
# accuracies <- rep(0, length(k_values))
# 
# # loop over different values of K, storing accuracies
# for (i in seq_along(k_values)) {
#   k <- k_values[i]
#   # pull each each knn
#   pull <- knn(train=train_1, test=valid_1, cl=train_label_vec, k=k, prob=TRUE)[1:length(valid_label_vec)]
#   
#   # create kable table
#   # tab <- kbl(table(truth=valid_label_vec,predicted=pull)) |>
#   #   kable_classic(full_width = F, html_font = "Cambria")
#   # # print each
#   # print(tab)
#   
#   # store each accuracy = predict == actual/total
#   acc <- sum(pull == valid_label_vec) / length(valid_label_vec)
#   accuracies[i] <- acc
# }
# 
# # create a df of results
# knn_accuracies <- data.frame(K=k_values, Accuracy=accuracies)
# 
# # max accuracy
# max_accuracy <- max(knn_accuracies$Accuracy)
# 
# # associated k 
# best_k <- knn_accuracies$K[which.max(knn_accuracies$Accuracy)]
# 
# # print
# cat("Best accuracy:\n")
# max_accuracy
# cat("K for best accuracy:\n")
# best_k
# 
# # create a plot
# ggplot(knn_accuracies, aes(x=K, y=Accuracy)) +
#   geom_line() +
#   geom_point() +
#   labs(x="K", y="Accuracy", title="KNN Classifier Accuracy for Different K Values, Scaled, Feature Reduced Set 1")
```

<!--# Investigating accuracy of models using KNN classifier with the reduced variable set shows a best accuracy of 94.1% with only one neighbour. We can investigate several reduced variable sets by setting up a loop and storing the results. -->

```{r loopKNN_datasets}

# loop through different datasets and values of K and return accuracies, plot

# define a list of data frames
train_dfs <- list(train_1, train_2, train_3, train_4, train_5, train_scaled)
valid_dfs <- list(valid_1, valid_2, valid_3, valid_4, valid_5, valid_scaled)

# loop over each data frame
for (j in seq_along(train_dfs)) {
  train_df <- train_dfs[[j]]
  valid_df <- valid_dfs[[j]]
  
  # loop through k, print confusion matrix and store accuracies
  
  # square root N of dataset to get upper K
  K = sqrt(length(train_df[,1]))
  
  k_values <- 1:K
  # empty vectors to store results
  accuracies <- rep(0, length(k_values))
  
  # loop over different values of K, storing accuracies
  for (i in seq_along(k_values)) {
    k <- k_values[i]
    # pull each each knn
    pull <- knn(train=train_df, test=valid_df, cl=train_label_vec, k=k, prob=TRUE)[1:length(valid_label_vec)]
    
    # create kable table
    # tab <- kbl(table(truth=valid_label_vec,predicted=pull)) |>
    #   kable_classic(full_width = F, html_font = "Cambria")
    # # print each
    # print(tab)
    
    # store each accuracy = predict == actual/total
    acc <- sum(pull == valid_label_vec) / length(valid_label_vec)
    accuracies[i] <- acc
  }
  
  # create a df of results
  knn_accuracies <- data.frame(K=k_values, Accuracy=accuracies)
  
  # max accuracy
  max_accuracy <- max(knn_accuracies$Accuracy)
  
  # associated k 
  best_k <- knn_accuracies$K[which.max(knn_accuracies$Accuracy)]

  # print
  cat(paste0("\nBest accuracy for valid_", j, ":\n"))
  print(max_accuracy)
  cat(paste0("K for best accuracy for valid_", j, ":\n"))
  print(best_k)

  print(ggplot(knn_accuracies, aes(x=K, y=Accuracy)) +
  geom_line() +
  geom_point() +
  labs(x="K", y="Accuracy", title="KNN Classifier Accuracy for Different K Values: ", j))

}
```

#### KNN loop {#sec-knn-loop}

<!--# The above KNN loop checks accuries for different datasets through different values of K.  Below, I have modified to store the results  -->

```{r knn_loop_combined, echo=FALSE, message = FALSE, tab.align='left', fig.alt="Plot KNN classifier accuracies for different K and datasets"}
#| label: fig-knn-accuracy-dataset
#| fig-cap: "KNN classifier accuracies for different K and datasets"

# modified above to store accuracies

# define a list of data frames
train_dfs <- list(train_1, train_2, train_3, train_4, train_5, train_scaled)
valid_dfs <- list(valid_1, valid_2, valid_3, valid_4, valid_5, valid_scaled)

# empty lists to store results
knn_accuracies_list <- list()
max_accuracy_list <- list()
best_k_list <- list()

# loop over each data frame
for (j in seq_along(train_dfs)) {
  train_df <- train_dfs[[j]]
  valid_df <- valid_dfs[[j]]
  
  # loop through k, print confusion matrix and store accuracies
  
  # square root N of dataset to get upper K
  K = sqrt(length(train_df[,1]))
  
  k_values <- 1:K
  # empty vectors to store results
  accuracies <- rep(0, length(k_values))
  
  # loop over different values of K, storing accuracies
  for (i in seq_along(k_values)) {
    k <- k_values[i]
    # pull each each knn
    pull <- knn(train=train_df, test=valid_df, cl=train_label_vec, k=k, prob=TRUE)[1:length(valid_label_vec)]
    
    # store each accuracy = predict == actual/total
    acc <- sum(pull == valid_label_vec) / length(valid_label_vec)
    accuracies[i] <- acc
  }
  
  # create a df of results
  knn_accuracies <- data.frame(K=k_values, Accuracy=accuracies)
  
  # max accuracy
  max_accuracy <- max(knn_accuracies$Accuracy)
  
  # associated k 
  best_k <- knn_accuracies$K[which.max(knn_accuracies$Accuracy)]

  # print
  cat(paste0("\nBest accuracy for valid_", j, ":\n"))
  print(max_accuracy)
  cat(paste0("K for best accuracy for valid_", j, ":\n"))
  print(best_k)
  
  # store results
  knn_accuracies_list[[j]] <- knn_accuracies
  max_accuracy_list[[j]] <- max_accuracy
  best_k_list[[j]] <- best_k
}

# plot the results
plot_df <- data.frame()
for (j in seq_along(train_dfs)) {
  plot_df <- rbind(plot_df, data.frame(K = knn_accuracies_list[[j]]$K,
                                       Accuracy = knn_accuracies_list[[j]]$Accuracy,
                                       Dataset = paste0("valid_", j)))
}

combined_knn_accuracies <- ggplot(plot_df, aes(x=K, y=Accuracy, color=Dataset, linetype=Dataset)) +
  geom_line() +
  geom_point() +
  labs(x="K", y="Accuracy", title="KNN Classifier Accuracy for Different K Values") +
  scale_color_discrete(name="Dataset") +
  scale_linetype_discrete(name="Dataset") +
  theme(legend.position="bottom")



# tables
last_table <- knn_accuracies_list[[6]]
third_table <- knn_accuracies_list[[3]]
# max accuracy
max_accuracy_full <- max(last_table$Accuracy)
max_valid_3 <- max(third_table$Accuracy)

round((max_accuracy_full*100),2)

```

The best performing KNN model uses `valid_3` consisting of: `r names(valid_3)`. The complete dataset (valid_6) peaks with an accuracy of `r round((max_accuracy_full*100),2)`% compared to `r round((max_valid_3*100),2)`% for the reduced variable set.

<!--# The best performing reduced variable dataset is set3 which contains the variables.  Its accuracy on the valid set is comparable to that of the full dataset. So how does it perform on the other models - mclust DA, SVM -->

```{r knn_all_accuracies_plot, include=TRUE, echo=FALSE, message = FALSE, tab.align='left', fig.alt="Plot KNN classifier accuracies for different K and datasets"}
#| label: fig-knn-all-accuracy-dataset
#| fig-cap: "KNN classifier accuracies for different K and datasets"


combined_knn_accuracies


```

```{r mclust_reduced_tune2}

# # confirmed as working  
# #try hypertune loop on valid_1 reduced set.
# 
# # hyperparameters to search
# n.components <- c(2, 3, 4, 5, 6)
# #n.components <- c(3, 4)
# diagonal <- c(TRUE, FALSE)
# modelName <- c("VEV", "EII", "VII", "EEE", "VVV", "EEI", "VEI", "EVI", "VVI", "VEE", "EVE", "VVE", "EEV", "VEV", "EVV"  )
# 
# 
# # grid to search
# hyperparameters <- expand.grid(n.components = n.components, diagonal = diagonal,modelName = modelName )
# 
# # initialise the best
# best.model <- NULL
# best.bic <- Inf
# 
# # loop over grid 
# for (i in 1:nrow(hyperparameters)) {
# 
#   # train_scaled to a numeric matrix
#   data <- as.matrix(train_1)
# 
#   # fit mclust model 
#   model <- MclustDA(data = data,
#                     class = train_label_vec,
#                     G = hyperparameters$n.components[i],
#                     diagonal = hyperparameters$diagonal[i],
#                     modelName = hyperparameters$modelName[i],
#                     verbose = FALSE)
#   
#   # bic
#   sum <- summary(model)
#   bic <- sum$bic
# 
#   # better bic, saved
#   if (bic < best.bic) {
#     best.model <- model
#     best.bic <- bic
#   }
# }
# 
# # best model
# sum_best_model2 <- summary(best.model, newdata = valid_1, newclass = valid_label_vec)
# 
# # results
# cat("Best BIC:\n")
# print(sum_best_model2$bic)
# cat("Model with best BIC:\n")
# print(sum_best_model2)
# 
# # make predictions on the validation set
# valid_preds2 <- predict(best.model, newdata = valid_1)
# valid_preds2 <- valid_preds2$classification
# 
# # calculate accuracy of predictions
# accuracy2 <- sum(valid_preds2 == valid_label_vec)/ length(valid_label_vec)
# 
# cat("\nBest accuracy (DA):\n")  
# accuracy2
```

<!--# As with knn, the feature reduced dataset performs comparably well with mclust DA.  Early indications are that the client can collect less data without much of a hit in terms of classification.  Below is a loop which uses accuracy as the measure instead of BIC. Very good results - 96% accuracy.-->

```{r loopthroughbestaccuracy}
# # confirmed as working - below has datasets incorporated
# # hyperparameters to search
# n.components <- c(2, 3, 4, 5, 6)
# diagonal <- c(TRUE, FALSE)
# modelName <- c("VEV", "EII", "VII", "EEE", "VVV", "EEI", "VEI", "EVI", "VVI", "VEE", "EVE", "VVE", "EEV", "VEV", "EVV")
# 
# # grid to search
# hyperparameters <- expand.grid(n.components = n.components,
#                                diagonal = diagonal,
#                                modelName = modelName)
# 
# # initialise the best
# best.model <- NULL
# best.accuracy <- 0
# 
# # loop over grid 
# for (i in 1:nrow(hyperparameters)) {
# 
#   # train_scaled to a numeric matrix
#   data <- as.matrix(train_1)
# 
#   # fit mclust model 
#   model <- MclustDA(data = data,
#                     class = train_label_vec,
#                     G = hyperparameters$n.components[i],
#                     diagonal = hyperparameters$diagonal[i],
#                     modelName = hyperparameters$modelName[i],
#                     verbose = FALSE)
#   
#   # make predictions on the validation set
#   valid_preds <- predict(model, newdata = valid_1)
#   valid_preds <- valid_preds$classification
# 
#   # calculate accuracy of predictions
#   accuracy <- sum(valid_preds == valid_label_vec)/length(valid_label_vec)
# 
#   # better accuracy, save model
#   if (accuracy > best.accuracy) {
#     best.model <- model
#     best.accuracy <- accuracy
#   }
# }
# 
# # best model
# sum_best_model <- summary(best.model, newdata = valid_1, newclass = valid_label_vec)
# 
# # print results
# cat("Best accuracy (DA):\n")  
# print(best.accuracy)
# cat("Model with best accuracy:\n")
# print(sum_best_model)
# 
# # make predictions on the validation set
# valid_preds <- predict(best.model, newdata = valid_1)
# valid_preds <- valid_preds$classification
# 
# # calculate accuracy of predictions
# accuracy <- sum(valid_preds == valid_label_vec)/ length(valid_label_vec)
# 
# cat("Accuracy of predictions on validation set (DA):\n")
# accuracy

```

<!--# These loops work but have been superseded by the combined loop below. -->

```{r loop_accu_var_set}
# # hyperparameters to search
# n.components <- c(2, 3, 4, 5, 6)
# diagonal <- c(TRUE, FALSE)
# modelName <- c("VEV", "EII", "VII", "EEE", "VVV", "EEI", "VEI", "EVI", "VVI", "VEE", "EVE", "VVE", "EEV", "VEV", "EVV")
# 
# # grid to search
# hyperparameters <- expand.grid(n.components = n.components,
#                                diagonal = diagonal,
#                                modelName = modelName)
# 
# # initialise the best
# best.model <- NULL
# best.accuracy <- 0
# best.df <- 0
# 
# # create a list of dataframes
# train_list <- list(train_1, train_2, train_3, train_4, train_5, train_scaled)
# valid_list <- list(valid_1, valid_2, valid_3, valid_4, valid_5, valid_scaled)
# 
# # loop over dataframes
# for (j in 1:length(train_list)) {
# 
#   # loop over grid 
#   for (i in 1:nrow(hyperparameters)) {
# 
#     # train_scaled to a numeric matrix
#     data <- as.matrix(train_list[[j]])
# 
#     # fit mclust model 
#     model <- MclustDA(data = data,
#                       class = train_label_vec,
#                       G = hyperparameters$n.components[i],
#                       diagonal = hyperparameters$diagonal[i],
#                       modelName = hyperparameters$modelName[i],
#                       verbose = FALSE)
#   
#     # make predictions on the validation set
#     valid_preds <- predict(model, newdata = valid_list[[j]])
#     valid_preds <- valid_preds$classification
# 
#     # calculate accuracy of predictions
#     accuracy <- sum(valid_preds == valid_label_vec)/length(valid_label_vec)
# 
#     # better accuracy, save model and dataframe index
#     if (accuracy > best.accuracy) {
#       best.model <- model
#       best.accuracy <- accuracy
#       best.df <- j
#     }
#   }
# }
# 
# # best model
# sum_best_model <- summary(best.model, newdata = valid_list[[best.df]], newclass = valid_label_vec)
# 
# # print results
# cat("Best accuracy (DA) for dataframe ", best.df, ":\n")  
# print(best.accuracy)
# cat("Model with best accuracy for dataframe ", best.df, ":\n")
# print(sum_best_model)
# 
# # make predictions on the validation set
# valid_preds <- predict(best.model, newdata = valid_list[[best.df]])
# valid_preds <- valid_preds$classification
# 
# # calculate accuracy of predictions
# accuracy <- sum(valid_preds == valid_label_vec)/ length(valid_label_vec)
# 
# cat("Accuracy of predictions on validation set (DA) for dataframe ", best.df, ":\n")
# accuracy
```

#### DA loop {#sec-da-loop}

Discriminant Analysis (LDA) models were tuned with 'n.components', 'diagonal' and 'model name' `hyperparameters` to allow the algorithm to find the combination of mixture models with the best accuracy for the groups.

```{r hyperloop_DA_full}
# 
# # combined loop which searches over hyperparameters and datasets - it takes a while!
# 
# # hyperparameters to iterate over
# n.components <- c(2, 3, 4, 5, 6)
# diagonal <- c(TRUE, FALSE)
# modelName <- c("VEV", "EII", "VII", "EEE", "VVV", "EEI", "VEI", "EVI", "VVI", "VEE", "EVE", "VVE", "EEV", "EVV")
# #modelName <- c("VEV", "EII")
# 
# # grid to search
# hyperparameters <- expand.grid(n.components = n.components,
#                                diagonal = diagonal,
#                                modelName = modelName)
# 
# # initialise the best
# best.model <- NULL
# best.accuracy <- 0
# best.df <- 0
# 
# # list to store the model hyperparameters and accuracy
# model_list <- list()
# 
# # list of dataframes
# train_list <- list(train_1, train_2, train_3, train_4, train_5, train_scaled)
# valid_list <- list(valid_1, valid_2, valid_3, valid_4, valid_5, valid_scaled)
# 
# # data frame to store accuracies
# accuracy_DA_df <- data.frame(dataset = character(),
#                            accuracy = numeric())
# 
# # loop over dataframes
# for (j in 1:length(train_list)) {
# 
#   # loop over grid 
#   for (i in 1:nrow(hyperparameters)) {
# 
#     # train_scaled to a numeric matrix
#     data <- as.matrix(train_list[[j]])
# 
#     # fit mclust model 
#     model <- MclustDA(data = data,
#                       class = train_label_vec,
#                       G = hyperparameters$n.components[i],
#                       diagonal = hyperparameters$diagonal[i],
#                       modelName = hyperparameters$modelName[i],
#                       verbose = FALSE)
#   
#     # make predictions on the validation set
#     valid_preds <- predict(model, newdata = valid_list[[j]])
#     valid_preds <- valid_preds$classification
# 
#     # calculate accuracy of predictions
#     accuracy <- sum(valid_preds == valid_label_vec)/length(valid_label_vec)
# 
#     # add accuracy and model parameters to list
#     model_list[[paste0(j, "_", i)]] <- list(dataset = j,
#                                              data = data,
#                                              n.components = hyperparameters$n.components[i],
#                                              diagonal = hyperparameters$diagonal[i],
#                                              modelName = hyperparameters$modelName[i],
#                                              accuracy = accuracy)
# 
#     # add accuracy to data frame
#     accuracy_DA_df <- rbind(accuracy_DA_df, data.frame(dataset = j, accuracy = accuracy))
# 
#     # better accuracy, save model and dataframe index
#     if (accuracy > best.accuracy) {
#       best.accuracy <- accuracy
#       best.df <- j
#     }
#   }
# }
# 
# # sort accuracy data frame by accuracy in descending order
# accuracy_DA_df <- accuracy_DA_df[order(accuracy_DA_df$accuracy, decreasing = TRUE),]
# 
# # sort model list by accuracy in descending order
# model_list <- model_list[order(sapply(model_list, function(x) x$accuracy), decreasing = TRUE)]
# 
# # print results - best overall
# cat("Best accuracy (DA) for dataframe ", best.df, ":\n")  
# print(best.accuracy)
# 
# cat("Model hyperparameters with best accuracy for dataframe ", best.df, ":\n")
# print(model_list[[1]][c("n.components", "diagonal", "modelName")])
# 
# # print top 10 - they are not distinct!
# cat("Top 10 models:\n")
# for (i in 1:10) {
# cat("Model ", i, "\n")
# cat("Dataset: ", model_list[[i]]$dataset, "\n")
# cat("Accuracy: ", model_list[[i]]$accuracy, "\n")
# cat("Model parameters:\n")
# print(summary(model_list[[i]]$model, newdata = valid_list[[model_list[[i]]$dataset]], newclass = valid_label_vec))
# cat("\n")
# }


```

```{r hyperloop2_DA}

# combined loop which searches over hyperparameters and datasets - it takes a while!
# v2 saves models

# hyperparameters to iterate over
n.components <- c(2, 3, 4, 5, 6)
diagonal <- c(TRUE, FALSE)
modelName <- c("VEV", "EII", "VII", "EEE", "VVV", "EEI", "VEI", "EVI", "VVI", "VEE", "EVE", "VVE", "EEV", "EVV")
#modelName <- c("VEV", "EII")

# grid to search
hyperparameters <- expand.grid(n.components = n.components,
                               diagonal = diagonal,
                               modelName = modelName)

# initialise the best
best.model <- NULL
best.accuracy <- 0
best.df <- 0

# list to store the model hyperparameters and accuracy
model_list <- list()

# list of dataframes
train_list <- list(train_1, train_2, train_3, train_4, train_5, train_scaled)
valid_list <- list(valid_1, valid_2, valid_3, valid_4, valid_5, valid_scaled)

# data frame to store accuracies
accuracy_DA_df <- data.frame(dataset = character(),
                           accuracy = numeric())

# loop over dataframes
for (j in 1:length(train_list)) {

  # loop over grid 
  for (i in 1:nrow(hyperparameters)) {

    # train_scaled to a numeric matrix
    data <- as.matrix(train_list[[j]])

    # fit mclust model 
    model <- MclustDA(data = data,
                      class = train_label_vec,
                      G = hyperparameters$n.components[i],
                      diagonal = hyperparameters$diagonal[i],
                      modelName = hyperparameters$modelName[i],
                      verbose = FALSE)
  
    # make predictions on the validation set
    valid_preds <- predict(model, newdata = valid_list[[j]])
    valid_preds <- valid_preds$classification

    # calculate accuracy of predictions
    accuracy <- sum(valid_preds == valid_label_vec)/length(valid_label_vec)

    # add accuracy and model parameters to list
    model_list[[paste0(j, "_", i)]] <- list(dataset = j,
                                             data = data,
                                             n.components = hyperparameters$n.components[i],
                                             diagonal = hyperparameters$diagonal[i],
                                             modelName = hyperparameters$modelName[i],
                                             accuracy = accuracy,
                                             model = model)

    # add accuracy to data frame
    accuracy_DA_df <- rbind(accuracy_DA_df, data.frame(dataset = j, accuracy = accuracy))

    # better accuracy, save model and dataframe index
    if (accuracy > best.accuracy) {
      best.accuracy <- accuracy
      best.df <- j
    }
  }
}

# sort accuracy data frame by accuracy in descending order
accuracy_DA_df <- accuracy_DA_df[order(accuracy_DA_df$accuracy, decreasing = TRUE),]

# sort model list by accuracy in descending order
model_list <- model_list[order(sapply(model_list, function(x) x$accuracy), decreasing = TRUE)]

# print results - best overall
cat("Best accuracy (DA) for dataframe ", best.df, ":\n")  
print(best.accuracy)

cat("Model hyperparameters with best accuracy for dataframe ", best.df, ":\n")
print(model_list[[1]][c("n.components", "diagonal", "modelName")])

# print top 10 - they are not distinct!
# cat("Top 10 models:\n")
# for (i in 1:3) {
#   cat("Model ", i, "\n")
#   cat("Dataset: ", model_list[[i]]$dataset, "\n")
#   cat("Accuracy: ", model_list[[i]]$accuracy, "\n")
#   cat("Model parameters:\n")
#   print(summary(model_list[[i]]$model, newdata = valid_list[[model_list[[i]]$dataset]], newclass = valid_label_vec))
#   cat("\n")
#}
```



The ten best DA models are:

```{r DA_acc_FT, include=TRUE, echo=FALSE, results='asis'}
#| tab.id: tbl-DA-accuracies-Top10
#| tbl-cap: "Top 10 Model-Dataset DA Models by Accuracy"
#| tbl-cap-location: bottom
#| tbl-alt: "Table with top 10 Model-Dataset DA Models by Accuracy"

# save results in flextable of top
DA_acc_FT <- accuracy_DA_df |> 
  mutate(dataset = case_when(
    dataset == 6 ~ "Complete",
    dataset == 3 ~ "Reduced Set 3",
    dataset == 4 ~ "Reduced Set 4",
    dataset == 2 ~ "Reduced Set 2",
    dataset == 1 ~ "Reduced Set 1",
    dataset == 5 ~ "Reduced Set 5",
    TRUE ~ as.character(dataset)
  )) |> 
  distinct(dataset, accuracy, .keep_all = TRUE) |> # loop above stores duplicate results
  arrange(desc(accuracy)) |> 
  head(10) |> 
  flextable() |> 
  autofit()

# highlight row
#DA_acc_FT 
#<- highlight(DA_acc_FT, j = "dataset", i = ~ dataset != "Complete Scaled", color = "yellow")

# print
DA_acc_FT

# get max accuracies, aggregated by dataset
max_accuracy_all <- accuracy_DA_df %>%
  group_by(dataset) %>%
  summarise(max_accuracy = max(accuracy)) %>%
  mutate(dataset_name = case_when(
    dataset == 6 ~ "Complete",
    dataset == 3 ~ "Reduced Set 3",
    dataset == 4 ~ "Reduced Set 4",
    dataset == 2 ~ "Reduced Set 2",
    dataset == 1 ~ "Reduced Set 1",
    dataset == 5 ~ "Reduced Set 5",
    TRUE ~ as.character(dataset)
  ))

# highlight rows
max_accuracy_all_FT <- max_accuracy_all |>
  flextable() 


# max complete data accuracy 
max_DA_complete <- round(100*max_accuracy_all[max_accuracy_all$dataset == 6, "max_accuracy"],2)
# max reduced data accuracy
max_DA_reduced <- round(100*max(max_accuracy_all[max_accuracy_all$dataset != 6, "max_accuracy"]),2)

max_DA_diff <- round((max_DA_complete - max_DA_reduced),2)


```

The complete dataset has the highest accuracy (`r max_DA_complete`%) but there are 4 models using reduced datasets in the top 10.  Set 3 has an accuracy of `r max_DA_reduced`% - a performance difference of `r max_DA_diff`% using only `r ncol(valid_3)` variables instead of `r ncol(train_scaled)`.

```{r DA_top10}

# save top 10 models
best_models <- list()
for (i in 1:50) {
  best_models[[i]] <- model_list[[i]]
}

# save  to a file (RDS is compressed)
saveRDS(best_models, "../report/best_models.RDS")
#save(best_models, file = "..report/best_models.RData")

# load all saved models
load_models <- readRDS("../report/best_models.RDS")
#load("../report/best_models.RData")

# print accuracy of the models
# for (i in 1:length(best_models)) {
# print(paste0("Model ", i, ": ", best_models[[i]]$dataset, ", Accuracy: ", best_models[[i]]$accuracy))
# }


# to remove duplicates
unique_models <- best_models <- best_models[order(sapply(best_models, function(x) x$accuracy), decreasing = TRUE)]

for (i in 1:length(unique_models)) {
  if (i == 1 || unique_models[[i]]$dataset != unique_models[[i-1]]$dataset) {
    model_info <- paste0("Model ", i, ": ", unique_models[[i]]$dataset, ", Accuracy: ", unique_models[[i]]$accuracy)
    print(model_info)
  }
  
  model_name <- paste0("model_", i, "_", unique_models[[i]]$dataset)
  assign(model_name, list(model = unique_models[[i]]$model, 
                          dataset = unique_models[[i]]$dataset, 
                          accuracy = unique_models[[i]]$accuracy))
}





```

```{r save_DA_models}


# indices of the models to extract and save
model_indices <- c(1, 7)

# loop through indices, save models to files
for (i in model_indices) {
  # unique name for model file
  model_name <- paste0("DA_model_", i, ".RDS")
  
  # model from model_list
  model <- model_list[[i]]$model
  
  # model to file
  saveRDS(model, file.path("../report", model_name))
}

# test file
# # load
# DA_model_1 <- readRDS("../report/DA_model_1.RDS")
# 
# # predictions using the saved model
# predictions <- predict(DA_model_1, newdata = valid_scaled, type = "class")


```

#### Random Forest loop {#sec--random-forest-loop-}



```{r randomforest_loop}

# add labels to reduced variable dataframes

train_1_rf <- train_scaled_label[, c("X7", "X8", "X9", "X10", "label")]
train_2_rf <- cbind(train_1_rf, train_scaled_label[, c("X3", "X1")])
train_3_rf <- cbind(train_1_rf, train_scaled_label[, c("X3", "X11")])
train_4_rf <- cbind(train_2_rf, train_scaled_label[, c("X2", "X13")])
train_5_rf <- cbind(train_3_rf, train_scaled_label[["X16"]])
colnames(train_5_rf)[ncol(train_5_rf)] <- "X16"

#colnames(train_scaled_label)
# valid_scaled

valid_1_rf <- valid_scaled_label[, c("X7", "X8", "X9", "X10", "label")]
valid_2_rf <- cbind(valid_1_rf, valid_scaled_label[, c("X3", "X1")])
valid_3_rf <- cbind(valid_1_rf, valid_scaled_label[, c("X3", "X11")])
valid_4_rf <- cbind(valid_2_rf, valid_scaled_label[, c("X2", "X13")])
valid_5_rf <- cbind(valid_3_rf, valid_scaled_label[["X16"]])
colnames(valid_5_rf)[ncol(valid_5_rf)] <- "X16"

train_list_rf <- list(train_1_rf, train_2_rf, train_3_rf, train_4_rf, train_5_rf, train_scaled_label)
valid_list_rf <- list(valid_1_rf, valid_2_rf, valid_3_rf, valid_4_rf, valid_5_rf, valid_scaled_label)

# dataframe for results
results <- data.frame(dataset = character(), accuracy = numeric())

# loop through datasets
for (i in 1:length(train_list_rf)) {
  train <- train_list_rf[[i]]
  valid <- valid_list_rf[[i]]
  
  # train model, with labels
  rf_default <- randomForest(as.factor(label) ~ ., data = train)
  
  # predictions on validation set
  pred_valid <- predict(rf_default, valid)
  
  # accuracy
  accuracy_rf_default <- sum(pred_valid == valid$label) / nrow(valid)
  
  # save results in dataframe
  results[i, "dataset"] <- paste0("Dataset_", i)
  results[i, "accuracy"] <- accuracy_rf_default
  
  # print results
  cat(paste0("Accuracy of random forest (default) on variable set ", i, ": ", round(accuracy_rf_default,3), "\n\n"))
}


round(max(results$accuracy*100),2)
round(min(results$accuracy*100),2)
results$dataset[which(results$accuracy == max(results$accuracy), arr.ind = TRUE)[1]]

```

The random forest classifiers are very quick to run and perform very well. All of the forests achieve higher than `r round(min(results$accuracy*100),2)`% accuracy with the complete dataset marginally best with an accuracy of `r round(max(results$accuracy*100),2)`%.

<!--# And the winner is mclustDA with 97% -->
